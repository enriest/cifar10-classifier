{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enriqueestevezalvarez/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries for the CIFAR-10 CNN project\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tarfile\n",
    "import torch\n",
    "import torchvision\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from plotly import graph_objects as go\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Configure device for training (GPU if available, otherwise CPU)\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d508bf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already extracted.\n"
     ]
    }
   ],
   "source": [
    "# Download and Extract CIFAR-10 Dataset\n",
    "\n",
    "# CIFAR-10 dataset URL from official source (University of Toronto)\n",
    "cifar10_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "# Create directory structure for storing the dataset\n",
    "data_dir = Path(\"cifar10_data\")  # Main directory for CIFAR-10 data\n",
    "archive_path = data_dir / \"cifar-10-python.tar.gz\"  # Path where compressed file will be saved\n",
    "\n",
    "# Create the data directory if it doesn't exist\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Check if the dataset archive has already been downloaded\n",
    "if not archive_path.exists():\n",
    "    print(\"Downloading CIFAR-10 dataset...\")\n",
    "    # Stream download to handle large files efficiently\n",
    "    response = requests.get(cifar10_url, stream=True)\n",
    "    \n",
    "    # Write the downloaded content to file in chunks (8KB at a time)\n",
    "    with open(archive_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# Define where the extracted files should go\n",
    "extract_dir = data_dir / \"cifar-10-batches-py\"\n",
    "\n",
    "# Check if the dataset has already been extracted\n",
    "if not extract_dir.exists():\n",
    "    print(\"Extracting CIFAR-10 dataset...\")\n",
    "    # Extract the tar.gz archive\n",
    "    with tarfile.open(archive_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=data_dir)  # Extract all files to data_dir\n",
    "    print(\"Extraction complete.\")\n",
    "else:\n",
    "    print(\"Dataset already extracted.\")\n",
    "\n",
    "# At this point, we have:\n",
    "# - cifar10_data/ (main directory)\n",
    "#   - cifar-10-python.tar.gz (compressed archive)\n",
    "#   - cifar-10-batches-py/ (extracted dataset with batch files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95003021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Transform to tensor for normalisation for test set\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# Using values different from ImageNet Stats, calculated with Claude Sonnet 4.5\n",
    "\n",
    "# Transform for the train set (augmentation+normalization)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(30),  # Rotate image randomly\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Random color adjustments\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# Download and load CIFAR-10\n",
    "trainset = datasets.CIFAR10(root=str(data_dir), train=True, transform=transform_train)\n",
    "testset = datasets.CIFAR10(root=str(data_dir), train=False, transform=transform)\n",
    "\n",
    "img, _ = trainset[0]  # Get the first image and its label from the trainset\n",
    "img_tensor = img\n",
    "print(img_tensor.shape)  # Shape: (C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e234bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.48679847..1.5390869].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frog bird automobile dog\n"
     ]
    }
   ],
   "source": [
    "import torchvision as torchvision\n",
    "\n",
    "# Visualize Sample Images from the CIFAR-10 Training Set\n",
    "\n",
    "# Define the list of class names from the trainset\n",
    "classes = trainset.classes\n",
    "\n",
    "def imshow(img):\n",
    "    # Unnormalize the image (reverse the normalization applied during preprocessing)\n",
    "    img = img / 2 + 0.5\n",
    "    # Convert the tensor to a NumPy array for plotting\n",
    "    npimg = img.numpy()\n",
    "    # Rearrange dimensions from (C, H, W) to (H, W, C) for matplotlib\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of 4 random training images and their labels using DataLoader\n",
    "dataiter = iter(DataLoader(trainset, batch_size=4, shuffle=True))\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Display the images in a grid\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# Print the class names for each image in the batch\n",
    "print(' '.join(f'{classes[labels[j]]}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# conv1d = nn.Conv1d(\n",
    "#     in_channels=3, # number of input channels e.g. a colored image has 3 channels, the output of a conv layer with 64 neurons has 64 channels\n",
    "#     out_channels=16, # the number of output chennels corresponds to the number of filters or number of neurons on the layers\n",
    "#     kernel_size=5, # the width and height of each convolution filter\n",
    "#     stride=1, # the number of steps taken for each movement of the filter\n",
    "#     padding=0 # the number of padding cells around the input\n",
    "# )\n",
    "\n",
    "# Let's create the convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f4459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv1: torch.Size([1, 32, 32, 32])\n",
      "After conv2: torch.Size([1, 32, 8, 8])\n",
      "After conv3: torch.Size([1, 64, 8, 8])\n",
      "After conv4: torch.Size([1, 64, 2, 2])\n",
      "After flatten: torch.Size([1, 512])\n",
      "After fc: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Define a Convolutional Neural Network (CNN) class. \n",
    "# 4 layers, with an approach of 1.5 between the 3rd and 4th layers, so it increases channels progressively\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # First convolutional layer: input 3 channels (RGB), output 32 feature maps\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  \n",
    "        \n",
    "        # Second convolutional layer: input 32, output 32 feature maps\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # Third convolutional layer: input 64, output 64 feature maps\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Fourth convolutional layer: input 96, output 128 feature maps\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "\n",
    "        # Adding new layers for better perfomance\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "\n",
    "        # Flatten layer to convert 2D feature maps to 1D feature vector\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Placeholder fully connected layer, upadted after flatten and 10 classes\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        # Sigmoid activation for output (for binary classification)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        ## Not using sigmoid since it has 10 classes and it's not binary\n",
    "    \n",
    "    def forward(self, x, verbose=False):\n",
    "        # Pass input through first conv layer and apply ReLU activation\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        if verbose: print(\"After conv1:\", x.shape)\n",
    "        # Second conv layer + ReLU\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        if verbose: print(\"After conv2:\", x.shape)\n",
    "        # Third conv layer + ReLU\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        if verbose: print(\"After conv3:\", x.shape)\n",
    "        # Fourth conv layer + ReLU\n",
    "        x = torch.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        if verbose: print(\"After conv4:\", x.shape)\n",
    "\n",
    "        x = torch.relu(self.bn5(self.conv5(x)))\n",
    "        x = torch.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool3(x)\n",
    "        #x = self.dropout3(x)\n",
    "\n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout4(x)\n",
    "        if verbose: print(\"After flatten:\", x.shape)\n",
    "        x = self.fc2(x)\n",
    "        if verbose: print(\"After fc:\", x.shape)\n",
    "        return x\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return f\"Total params: {total_params:,}, Trainable: {trainable_params:,}\"\n",
    "\n",
    "# ---- Utility to automatically adjust the Linear layer ---- This is the utility function ----\n",
    "def initialize_model(input_shape=(3, 32, 32)):\n",
    "    # Create a dummy input tensor with the given shape\n",
    "    dummy = torch.zeros(1, *input_shape)  # batch size 1\n",
    "    model = CNN()\n",
    "    # Forward pass through conv layers to determine flatten size\n",
    "    with torch.no_grad():\n",
    "        out = model.conv1(dummy)\n",
    "        out = torch.relu(model.bn1(out))\n",
    "        out = model.conv2(out)\n",
    "        out = torch.relu(model.bn2(out))\n",
    "        out = model.pool1(out)\n",
    "        out = model.dropout1(out)\n",
    "        out = model.conv3(out)\n",
    "        out = torch.relu(model.bn3(out))\n",
    "        out = model.conv4(out)\n",
    "        out = torch.relu(model.bn4(out))\n",
    "        out = model.pool2(out)\n",
    "        out = model.dropout2(out)\n",
    "        out = model.conv5(out)\n",
    "        out = torch.relu(model.bn5(out))\n",
    "        out = model.conv6(out)\n",
    "        out = torch.relu(model.bn6(out))\n",
    "        out = model.pool3(out)\n",
    "        #out = model.dropout3(out)\n",
    "        out = model.flatten(out)\n",
    "        flatten_size = out.shape[1]\n",
    "    # Redefine the fully connected layer with the correct input size\n",
    "    model.fc1 = nn.Linear(flatten_size, 512)\n",
    "    model.fc2 = nn.Linear(512, 10)\n",
    "    return model\n",
    "\n",
    "# Example: initialize for 32x32 input images (to match the test input below)\n",
    "model = initialize_model(input_shape=(3, 32, 32))\n",
    "\n",
    "# Test forward with shape debugging\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "_ = model(x, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f42c8f",
   "metadata": {},
   "source": [
    "## **Why Use Dropout, Pooling, and Batch Normalization?**\n",
    "\n",
    "Let me explain each technique and how they help your CNN:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Batch Normalization** üìä\n",
    "\n",
    "### **What it does:**\n",
    "Normalizes the inputs to each layer so they have mean‚âà0 and std‚âà1.\n",
    "\n",
    "### **Why it helps:**\n",
    "- **Faster training:** Networks converge much quicker\n",
    "- **Stable gradients:** Prevents vanishing/exploding gradients\n",
    "- **Less sensitive to initialization:** More robust training\n",
    "- **Slight regularization effect:** Reduces overfitting a bit\n",
    "\n",
    "### **In your model:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce507b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Result:** Your model trains faster and more reliably!\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Dropout** üé≤\n",
    "\n",
    "### **What it does:**\n",
    "Randomly sets 50% of neurons to zero during training.\n",
    "\n",
    "### **Why it helps:**\n",
    "- **Prevents overfitting:** Forces model to not rely on specific neurons\n",
    "- **Better generalization:** Model learns more robust features\n",
    "- **Reduces memorization:** Can't memorize training data patterns\n",
    "\n",
    "### **In your model:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821a013f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Result:** Your model performs better on new, unseen data!\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Pooling** üèä‚Äç‚ôÇÔ∏è\n",
    "\n",
    "### **What it does:**\n",
    "Reduces spatial dimensions by taking max/average values in small regions.\n",
    "\n",
    "### **Why it helps:**\n",
    "- **Reduces parameters:** Fewer computations needed\n",
    "- **Translation invariance:** Object recognition regardless of position\n",
    "- **Prevents overfitting:** Less detailed features = less memorization\n",
    "- **Computational efficiency:** Smaller feature maps = faster processing\n",
    "\n",
    "### **You're NOT using pooling currently:**\n",
    "Instead, you use **stride=2** which achieves similar dimension reduction:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e12e52",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Visual Example:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159beec",
   "metadata": {},
   "source": [
    "Input: 32√ó32√ó3 image\n",
    "     ‚Üì conv1 + bn1 + relu (stride=2)\n",
    "   16√ó16√ó32 ‚Üê Batch norm stabilizes training\n",
    "     ‚Üì conv2 + bn2 + relu (stride=2)  \n",
    "   8√ó8√ó64   ‚Üê More batch norm\n",
    "     ‚Üì conv3 + bn3 + relu (stride=2)\n",
    "   4√ó4√ó96   ‚Üê Even more batch norm  \n",
    "     ‚Üì conv4 + bn4 + relu (stride=2)\n",
    "   2√ó2√ó128  ‚Üê Final batch norm\n",
    "     ‚Üì flatten\n",
    "   512      ‚Üê 1D vector\n",
    "     ‚Üì dropout (50% neurons zeroed)\n",
    "   512      ‚Üê Some neurons randomly off\n",
    "     ‚Üì fully connected\n",
    "   10       ‚Üê Final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be723df",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Summary:**\n",
    "- **Batch Norm:** Makes training faster and more stable\n",
    "- **Dropout:** Prevents overfitting, improves generalization  \n",
    "- **Pooling (or stride=2):** Reduces size, adds translation invariance\n",
    "\n",
    "**Your current approach is excellent!** You're using batch norm and dropout effectively, and stride=2 instead of pooling works great too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538fb07c",
   "metadata": {},
   "source": [
    "After this comment line, there is a **utility function** that automatically calculates the correct input size for the fully connected (Linear) layer.\n",
    "\n",
    "## **The Problem:**\n",
    "When you design a CNN, after all the convolutional and pooling layers, you need to flatten the output and connect it to a fully connected layer. But it's hard to manually calculate what size the flattened output will be after all those transformations.\n",
    "\n",
    "## **The Solution:**\n",
    "The `initialize_model()` function:\n",
    "1. **Creates a dummy input** with the specified shape (e.g., 3√ó100√ó100 for a color image)\n",
    "2. **Runs it through all the convolutional layers** to see what size comes out\n",
    "3. **Calculates the flattened size** (how many numbers you get after flattening)\n",
    "4. **Updates the Linear layer** to have the correct input size\n",
    "\n",
    "## **Why This is Useful:**\n",
    "Instead of manually calculating \"after 4 conv layers with stride 2, my 32√ó32 image becomes X√óX size,\" this function does it automatically by actually running a test input through the network and measuring the result.\n",
    "\n",
    "This ensures your fully connected layer has the right input size to connect properly with the convolutional layers above it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce5ec5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN                                      [1, 10]                   --\n",
       "‚îú‚îÄConv2d: 1-1                            [1, 32, 32, 32]           896\n",
       "‚îú‚îÄBatchNorm2d: 1-2                       [1, 32, 32, 32]           64\n",
       "‚îú‚îÄConv2d: 1-3                            [1, 32, 16, 16]           9,248\n",
       "‚îú‚îÄBatchNorm2d: 1-4                       [1, 32, 16, 16]           64\n",
       "‚îú‚îÄMaxPool2d: 1-5                         [1, 32, 8, 8]             --\n",
       "‚îú‚îÄDropout: 1-6                           [1, 32, 8, 8]             --\n",
       "‚îú‚îÄConv2d: 1-7                            [1, 64, 8, 8]             18,496\n",
       "‚îú‚îÄBatchNorm2d: 1-8                       [1, 64, 8, 8]             128\n",
       "‚îú‚îÄConv2d: 1-9                            [1, 64, 4, 4]             36,928\n",
       "‚îú‚îÄBatchNorm2d: 1-10                      [1, 64, 4, 4]             128\n",
       "‚îú‚îÄMaxPool2d: 1-11                        [1, 64, 2, 2]             --\n",
       "‚îú‚îÄDropout: 1-12                          [1, 64, 2, 2]             --\n",
       "‚îú‚îÄConv2d: 1-13                           [1, 128, 2, 2]            73,856\n",
       "‚îú‚îÄBatchNorm2d: 1-14                      [1, 128, 2, 2]            256\n",
       "‚îú‚îÄConv2d: 1-15                           [1, 128, 2, 2]            147,584\n",
       "‚îú‚îÄBatchNorm2d: 1-16                      [1, 128, 2, 2]            256\n",
       "‚îú‚îÄMaxPool2d: 1-17                        [1, 128, 1, 1]            --\n",
       "‚îú‚îÄFlatten: 1-18                          [1, 128]                  --\n",
       "‚îú‚îÄLinear: 1-19                           [1, 512]                  66,048\n",
       "‚îú‚îÄDropout: 1-20                          [1, 512]                  --\n",
       "‚îú‚îÄLinear: 1-21                           [1, 10]                   5,130\n",
       "==========================================================================================\n",
       "Total params: 359,082\n",
       "Trainable params: 359,082\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 6.02\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.76\n",
       "Params size (MB): 1.44\n",
       "Estimated Total Size (MB): 2.21\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(1, 3, 32, 32))  # (batch_size, channels, height, width)\n",
    "# the input size is the same as the default by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0bc0f",
   "metadata": {},
   "source": [
    "Now that the model is defined we need to define the loss function, the optimizer, and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for early stopping\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "        \"\"\"\n",
    "        Early stopping to stop training when validation loss stops improving.\n",
    "        \n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last improvement\n",
    "            min_delta (float): Minimum change to qualify as an improvement\n",
    "            restore_best_weights (bool): Whether to restore model weights from best epoch\n",
    "        \"\"\"\n",
    "        self.patience = patience  # Number of epochs to wait before stopping\n",
    "        self.min_delta = min_delta  # Minimum improvement threshold\n",
    "        self.restore_best_weights = restore_best_weights  # Whether to restore best weights\n",
    "        self.best_loss = None  # Track the best validation loss seen so far\n",
    "        self.counter = 0  # Count epochs without improvement\n",
    "        self.best_weights = None  # Store the best model weights\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Check if training should stop and update internal state.\n",
    "        \n",
    "        Args:\n",
    "            val_loss (float): Current validation loss\n",
    "            model: The PyTorch model being trained\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if training should stop, False if it should continue\n",
    "        \"\"\"\n",
    "        # First epoch - initialize best loss and save weights\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "            \n",
    "        # Check if current loss is better than best loss (by at least min_delta)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            # Improvement found - update best loss, reset counter, save weights\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0  # Reset the patience counter\n",
    "            self.save_checkpoint(model)\n",
    "            \n",
    "        else:\n",
    "            # No improvement - increment counter\n",
    "            self.counter += 1\n",
    "            \n",
    "        # Check if we've exceeded patience limit\n",
    "        if self.counter >= self.patience:\n",
    "            # Restore best weights if requested\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True  # Signal to stop training\n",
    "            \n",
    "        return False  # Continue training\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"\n",
    "        Save the current model weights as the best checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            model: The PyTorch model to save weights from\n",
    "        \"\"\"\n",
    "        # Create a deep copy of the model's state dictionary\n",
    "        # This prevents issues if the original model weights change\n",
    "        self.best_weights = model.state_dict().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f0825af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Changing the model so it adds up the early stopping function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Adding a regularisation technique for avoiding overfitting. Handicap to the loss function.\n",
    "# This is light, common for CNNs\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "# Adding a scheduler. It adjusts automatiquely the learning rate during the training allowing us to use different learning rates in different moments\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=100, patience=5):\n",
    "    \"\"\"\n",
    "    Function to train a PyTorch model with training and validation datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    model: The neural network model to train.\n",
    "    train_loader: DataLoader for the training dataset.\n",
    "    val_loader: DataLoader for the validation dataset.\n",
    "    criterion: Loss function (e.g., Binary Cross Entropy for classification).\n",
    "    optimizer: Optimization algorithm (e.g., Adam, SGD).\n",
    "    epochs: Number of training epochs (default=100).\n",
    "    \n",
    "    Returns:\n",
    "    history: Dictionary containing loss and accuracy for both training and validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=0.001, restore_best_weights=True)\n",
    "\n",
    "    # Dictionary to store training & validation loss and accuracy over epochs\n",
    "    history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(epochs):  # Loop over the number of epochs\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss, correct = 0, 0  # Initialize total loss and correct predictions\n",
    "        \n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()  # Reset gradients before each batch\n",
    "            outputs = model(inputs)  # Forward pass, outputs shape: [batch, 10]\n",
    "            loss = criterion(outputs, labels)  # labels shape: [batch], int64\n",
    "            loss.backward()  # Backpropagation (compute gradients)\n",
    "            optimizer.step()  # Update model parameters\n",
    "            \n",
    "            total_loss += loss.item()  # Accumulate batch loss\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "        \n",
    "        # Compute average loss and accuracy for training\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation phase (without gradient computation)\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad():  # No need to compute gradients during validation\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                outputs = model(val_inputs)\n",
    "                loss = criterion(outputs, val_labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == val_labels).sum().item()\n",
    "        \n",
    "        # Compute average loss and accuracy for validation\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Store metrics in history dictionary\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        \n",
    "        # Print training progress\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "            # Save best model based on validation accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "            # Save the best model\n",
    "            torch.save({\n",
    "                'epoch': best_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc}, 'best_model_cifar10.pth')\n",
    "            \n",
    "            print(f\"üíæ New best model saved! Epoch {best_epoch}, Val Acc: {best_val_acc:.4f}\")\n",
    "\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(f\"üõë Early stopping triggered after {epoch+1} epochs\")\n",
    "            print(f\"Best validation loss: {early_stopping.best_loss:.4f}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Best model from epoch {best_epoch}\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    return history  # Return training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae3768d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded best model from epoch 21\n",
      "Best validation accuracy: 0.7337\n",
      "Best validation loss: 0.7550\n",
      "‚úÖ Model ready for evaluation!\n"
     ]
    }
   ],
   "source": [
    "def load_best_model(model_path='best_model_cifar10.pth'):\n",
    "    \"\"\"Load the best saved model\"\"\"\n",
    "    \n",
    "    # Check if the model file exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"‚ùå Model file '{model_path}' not found.\")\n",
    "        print(\"Please run the training cell first to create the saved model.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    # Create model instance (same architecture)\n",
    "    model = initialize_model(input_shape=(3, 32, 32))\n",
    "    \n",
    "    # Load the best weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    # Move to appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Print model info\n",
    "    print(f\"‚úÖ Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "    val_acc = checkpoint.get('best_val_acc', 0.0)\n",
    "    if val_acc != 0.0:\n",
    "        print(f\"Best validation accuracy: {val_acc:.4f}\")\n",
    "    else:\n",
    "        print(f\"Best validation accuracy: N/A\")\n",
    "    print(f\"Best validation loss: {checkpoint.get('best_val_loss', 'N/A'):.4f}\")\n",
    "    \n",
    "    return model, checkpoint\n",
    "\n",
    "# Usage - only try to load if the file exists\n",
    "if os.path.exists('best_model_cifar10.pth'):\n",
    "    best_model, checkpoint_info = load_best_model('best_model_cifar10.pth')\n",
    "    if best_model is not None:\n",
    "        print(\"‚úÖ Model ready for evaluation!\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to load model\")\n",
    "else:\n",
    "    print(\"üìù No saved model found. Run training first!\")\n",
    "    best_model, checkpoint_info = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb022979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 50,000\n",
      "Validation samples: 10,000\n",
      "Batch size: 64\n",
      "Training batches: 782\n",
      "Validation batches: 157\n",
      "Epoch [1/30], Loss: 1.6516, Acc: 0.3861, Val Loss: 1.3037, Val Acc: 0.5236, LR: 0.001000\n",
      "üíæ New best model saved! Epoch 1, Val Acc: 0.5236\n",
      "Epoch [1/30], Loss: 1.6516, Acc: 0.3861, Val Loss: 1.3037, Val Acc: 0.5236, LR: 0.001000\n",
      "üíæ New best model saved! Epoch 1, Val Acc: 0.5236\n",
      "Epoch [2/30], Loss: 1.4142, Acc: 0.4865, Val Loss: 1.1548, Val Acc: 0.5795, LR: 0.001000\n",
      "üíæ New best model saved! Epoch 2, Val Acc: 0.5795\n",
      "Epoch [2/30], Loss: 1.4142, Acc: 0.4865, Val Loss: 1.1548, Val Acc: 0.5795, LR: 0.001000\n",
      "üíæ New best model saved! Epoch 2, Val Acc: 0.5795\n",
      "Epoch [3/30], Loss: 1.3065, Acc: 0.5304, Val Loss: 1.0997, Val Acc: 0.5995, LR: 0.001000\n",
      "üíæ New best model saved! Epoch 3, Val Acc: 0.5995\n",
      "Epoch [3/30], Loss: 1.3065, Acc: 0.5304, Val Loss: 1.0997, Val Acc: 0.5995, LR: 0.001000\n",
      "üíæ New best model saved! Epoch 3, Val Acc: 0.5995\n",
      "Epoch [4/30], Loss: 1.2442, Acc: 0.5576, Val Loss: 0.9976, Val Acc: 0.6372, LR: 0.001000\n",
      "üíæ New best model saved! Epoch 4, Val Acc: 0.6372\n",
      "Epoch [4/30], Loss: 1.2442, Acc: 0.5576, Val Loss: 0.9976, Val Acc: 0.6372, LR: 0.001000\n",
      "üíæ New best model saved! Epoch 4, Val Acc: 0.6372\n",
      "Epoch [5/30], Loss: 1.1972, Acc: 0.5752, Val Loss: 0.9483, Val Acc: 0.6645, LR: 0.001000\n",
      "üíæ New best model saved! Epoch 5, Val Acc: 0.6645\n",
      "Epoch [5/30], Loss: 1.1972, Acc: 0.5752, Val Loss: 0.9483, Val Acc: 0.6645, LR: 0.001000\n",
      "üíæ New best model saved! Epoch 5, Val Acc: 0.6645\n",
      "Epoch [6/30], Loss: 1.1517, Acc: 0.5939, Val Loss: 0.9025, Val Acc: 0.6811, LR: 0.001000\n",
      "üíæ New best model saved! Epoch 6, Val Acc: 0.6811\n",
      "Epoch [6/30], Loss: 1.1517, Acc: 0.5939, Val Loss: 0.9025, Val Acc: 0.6811, LR: 0.001000\n",
      "üíæ New best model saved! Epoch 6, Val Acc: 0.6811\n",
      "Epoch [7/30], Loss: 1.1205, Acc: 0.6068, Val Loss: 0.9398, Val Acc: 0.6668, LR: 0.000100\n",
      "Epoch [7/30], Loss: 1.1205, Acc: 0.6068, Val Loss: 0.9398, Val Acc: 0.6668, LR: 0.000100\n",
      "Epoch [8/30], Loss: 1.0291, Acc: 0.6355, Val Loss: 0.8122, Val Acc: 0.7125, LR: 0.000100\n",
      "üíæ New best model saved! Epoch 8, Val Acc: 0.7125\n",
      "Epoch [8/30], Loss: 1.0291, Acc: 0.6355, Val Loss: 0.8122, Val Acc: 0.7125, LR: 0.000100\n",
      "üíæ New best model saved! Epoch 8, Val Acc: 0.7125\n",
      "Epoch [9/30], Loss: 1.0060, Acc: 0.6461, Val Loss: 0.7931, Val Acc: 0.7166, LR: 0.000100\n",
      "üíæ New best model saved! Epoch 9, Val Acc: 0.7166\n",
      "Epoch [9/30], Loss: 1.0060, Acc: 0.6461, Val Loss: 0.7931, Val Acc: 0.7166, LR: 0.000100\n",
      "üíæ New best model saved! Epoch 9, Val Acc: 0.7166\n",
      "Epoch [10/30], Loss: 0.9968, Acc: 0.6482, Val Loss: 0.7806, Val Acc: 0.7253, LR: 0.000100\n",
      "üíæ New best model saved! Epoch 10, Val Acc: 0.7253\n",
      "Epoch [10/30], Loss: 0.9968, Acc: 0.6482, Val Loss: 0.7806, Val Acc: 0.7253, LR: 0.000100\n",
      "üíæ New best model saved! Epoch 10, Val Acc: 0.7253\n",
      "Epoch [11/30], Loss: 0.9831, Acc: 0.6542, Val Loss: 0.7753, Val Acc: 0.7242, LR: 0.000100\n",
      "Epoch [11/30], Loss: 0.9831, Acc: 0.6542, Val Loss: 0.7753, Val Acc: 0.7242, LR: 0.000100\n",
      "Epoch [12/30], Loss: 0.9790, Acc: 0.6559, Val Loss: 0.7714, Val Acc: 0.7259, LR: 0.000100\n",
      "üíæ New best model saved! Epoch 12, Val Acc: 0.7259\n",
      "Epoch [12/30], Loss: 0.9790, Acc: 0.6559, Val Loss: 0.7714, Val Acc: 0.7259, LR: 0.000100\n",
      "üíæ New best model saved! Epoch 12, Val Acc: 0.7259\n",
      "Epoch [13/30], Loss: 0.9743, Acc: 0.6586, Val Loss: 0.7643, Val Acc: 0.7283, LR: 0.000100\n",
      "üíæ New best model saved! Epoch 13, Val Acc: 0.7283\n",
      "Epoch [13/30], Loss: 0.9743, Acc: 0.6586, Val Loss: 0.7643, Val Acc: 0.7283, LR: 0.000100\n",
      "üíæ New best model saved! Epoch 13, Val Acc: 0.7283\n",
      "Epoch [14/30], Loss: 0.9657, Acc: 0.6615, Val Loss: 0.7630, Val Acc: 0.7303, LR: 0.000010\n",
      "üíæ New best model saved! Epoch 14, Val Acc: 0.7303\n",
      "Epoch [14/30], Loss: 0.9657, Acc: 0.6615, Val Loss: 0.7630, Val Acc: 0.7303, LR: 0.000010\n",
      "üíæ New best model saved! Epoch 14, Val Acc: 0.7303\n",
      "Epoch [15/30], Loss: 0.9524, Acc: 0.6625, Val Loss: 0.7597, Val Acc: 0.7317, LR: 0.000010\n",
      "üíæ New best model saved! Epoch 15, Val Acc: 0.7317\n",
      "Epoch [15/30], Loss: 0.9524, Acc: 0.6625, Val Loss: 0.7597, Val Acc: 0.7317, LR: 0.000010\n",
      "üíæ New best model saved! Epoch 15, Val Acc: 0.7317\n",
      "Epoch [16/30], Loss: 0.9496, Acc: 0.6650, Val Loss: 0.7546, Val Acc: 0.7342, LR: 0.000010\n",
      "üíæ New best model saved! Epoch 16, Val Acc: 0.7342\n",
      "Epoch [16/30], Loss: 0.9496, Acc: 0.6650, Val Loss: 0.7546, Val Acc: 0.7342, LR: 0.000010\n",
      "üíæ New best model saved! Epoch 16, Val Acc: 0.7342\n",
      "Epoch [17/30], Loss: 0.9516, Acc: 0.6629, Val Loss: 0.7566, Val Acc: 0.7322, LR: 0.000010\n",
      "Epoch [17/30], Loss: 0.9516, Acc: 0.6629, Val Loss: 0.7566, Val Acc: 0.7322, LR: 0.000010\n",
      "Epoch [18/30], Loss: 0.9513, Acc: 0.6632, Val Loss: 0.7521, Val Acc: 0.7346, LR: 0.000010\n",
      "üíæ New best model saved! Epoch 18, Val Acc: 0.7346\n",
      "Epoch [18/30], Loss: 0.9513, Acc: 0.6632, Val Loss: 0.7521, Val Acc: 0.7346, LR: 0.000010\n",
      "üíæ New best model saved! Epoch 18, Val Acc: 0.7346\n",
      "Epoch [19/30], Loss: 0.9484, Acc: 0.6680, Val Loss: 0.7521, Val Acc: 0.7345, LR: 0.000010\n",
      "Epoch [19/30], Loss: 0.9484, Acc: 0.6680, Val Loss: 0.7521, Val Acc: 0.7345, LR: 0.000010\n",
      "Epoch [20/30], Loss: 0.9493, Acc: 0.6640, Val Loss: 0.7569, Val Acc: 0.7322, LR: 0.000010\n",
      "Epoch [20/30], Loss: 0.9493, Acc: 0.6640, Val Loss: 0.7569, Val Acc: 0.7322, LR: 0.000010\n",
      "Epoch [21/30], Loss: 0.9428, Acc: 0.6661, Val Loss: 0.7521, Val Acc: 0.7331, LR: 0.000001\n",
      "Epoch [21/30], Loss: 0.9428, Acc: 0.6661, Val Loss: 0.7521, Val Acc: 0.7331, LR: 0.000001\n",
      "Epoch [22/30], Loss: 0.9458, Acc: 0.6670, Val Loss: 0.7523, Val Acc: 0.7351, LR: 0.000001\n",
      "üíæ New best model saved! Epoch 22, Val Acc: 0.7351\n",
      "Epoch [22/30], Loss: 0.9458, Acc: 0.6670, Val Loss: 0.7523, Val Acc: 0.7351, LR: 0.000001\n",
      "üíæ New best model saved! Epoch 22, Val Acc: 0.7351\n",
      "Epoch [23/30], Loss: 0.9374, Acc: 0.6693, Val Loss: 0.7544, Val Acc: 0.7322, LR: 0.000001\n",
      "üõë Early stopping triggered after 23 epochs\n",
      "Best validation loss: 0.7521\n",
      "\n",
      "Training completed!\n",
      "Best model from epoch 22\n",
      "Best validation accuracy: 0.7351\n",
      "Best validation loss: 0.7523\n",
      "‚úÖ Loaded best model from epoch 22\n",
      "Best validation accuracy: 0.7351\n",
      "Best validation loss: 0.7523\n",
      "Epoch [23/30], Loss: 0.9374, Acc: 0.6693, Val Loss: 0.7544, Val Acc: 0.7322, LR: 0.000001\n",
      "üõë Early stopping triggered after 23 epochs\n",
      "Best validation loss: 0.7521\n",
      "\n",
      "Training completed!\n",
      "Best model from epoch 22\n",
      "Best validation accuracy: 0.7351\n",
      "Best validation loss: 0.7523\n",
      "‚úÖ Loaded best model from epoch 22\n",
      "Best validation accuracy: 0.7351\n",
      "Best validation loss: 0.7523\n"
     ]
    }
   ],
   "source": [
    "# TRAINING EXECUTION - Set up data loaders and train the model\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "# DataLoader handles batching, shuffling, and loading data efficiently\n",
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True)   # Training: shuffle for better learning\n",
    "val_loader = DataLoader(testset, batch_size=64, shuffle=False)     # Validation: no shuffle needed\n",
    "\n",
    "# Print dataset information for transparency and debugging\n",
    "print(f\"Training samples: {len(trainset):,}\")        # Total number of training images\n",
    "print(f\"Validation samples: {len(testset):,}\")       # Total number of validation images  \n",
    "print(f\"Batch size: 64\")                             # How many images processed at once\n",
    "print(f\"Training batches: {len(train_loader)}\")      # Number of batches in training set\n",
    "print(f\"Validation batches: {len(val_loader)}\")      # Number of batches in validation set\n",
    "\n",
    "# Execute the training process\n",
    "# This calls the train() function defined earlier with all necessary parameters\n",
    "history = train(\n",
    "    model,                    # The CNN model we defined and initialized\n",
    "    train_loader=train_loader,    # DataLoader for training data\n",
    "    val_loader=val_loader,        # DataLoader for validation data\n",
    "    criterion=criterion,          # Loss function (CrossEntropyLoss)\n",
    "    optimizer=optimizer,          # Optimization algorithm (Adam)\n",
    "    scheduler=scheduler,          # Learning rate scheduler (StepLR)\n",
    "    epochs=30,                   # Maximum number of training epochs\n",
    "    patience=5                   # Early stopping patience (stop if no improvement for 5 epochs)\n",
    ")\n",
    "\n",
    "# Load the best model that was automatically saved during training\n",
    "# The train() function saves the model with highest validation accuracy\n",
    "best_model, _ = load_best_model('best_model_cifar10.pth')\n",
    "\n",
    "# At this point:\n",
    "# - 'history' contains training metrics (loss, accuracy) for each epoch\n",
    "# - 'best_model' contains the model weights from the epoch with best validation accuracy\n",
    "# - The model is ready for evaluation and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b47e87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download reference/pre-trained model for comparison\n",
    "\n",
    "url=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M08-deep-learning/model_stride.pth\"\n",
    "output_path=\"model_stride.pth\"\n",
    "\n",
    "# Download the reference model (only if not already present)\n",
    "if not os.path.exists(output_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(output_path, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "    print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a74e52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference model loaded successfully!\n",
      "Reference model FC1 input features: 128\n"
     ]
    }
   ],
   "source": [
    "# Load the reference model for comparison\n",
    "\n",
    "\n",
    "ckpt = torch.load(\"model_stride.pth\", map_location=\"cpu\")\n",
    "# Use the correct key for the fully connected layer (fc1 or fc2)\n",
    "in_features = ckpt[\"model_state_dict\"][\"fc1.weight\"].shape[1]  # -> 2048 or similar\n",
    "\n",
    "# Create reference model with correct architecture\n",
    "restored_model = CNN()\n",
    "restored_model.fc1 = nn.Linear(in_features, 512)  # match architecture\n",
    "restored_model.fc2 = nn.Linear(512, 10)           # match architecture\n",
    "restored_model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "restored_model.eval()\n",
    "\n",
    "print(f\"Reference model loaded successfully!\")\n",
    "print(f\"Reference model FC1 input features: {in_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82f32ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL COMPARISON ===\n",
      "Your model parameters: 359,082\n",
      "Reference model parameters: 359,082\n",
      "Your model output shape: torch.Size([1, 10])\n",
      "Reference model output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Compare my trained model with the reference model\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(f\"Your model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Reference model parameters: {sum(p.numel() for p in restored_model.parameters()):,}\")\n",
    "\n",
    "# Test both models on a sample\n",
    "with torch.no_grad():\n",
    "    sample_input = torch.randn(1, 3, 32, 32)\n",
    "    your_output = model(sample_input)\n",
    "    ref_output = restored_model(sample_input)\n",
    "    \n",
    "    print(f\"Your model output shape: {your_output.shape}\")\n",
    "    print(f\"Reference model output shape: {ref_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ea941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE COMPREHENSIVE MODEL COMPARISON TABLE\n",
    "# This provides a clear side-by-side comparison of both models\n",
    "\n",
    "# Create comparison data dictionary with model metrics\n",
    "comparison_data = {\n",
    "    'Model': ['Custom CNN', 'MobileNetV2'],\n",
    "    'Parameters': [f\"{sum(p.numel() for p in model.parameters()):,}\", \n",
    "                   f\"{sum(p.numel() for p in mobilenet_v2.parameters()):,}\"],\n",
    "    'Accuracy': ['To be filled after training', 'To be filled after training'],  # Fill with actual values after training\n",
    "    'Architecture': ['6-layer CNN with BatchNorm + Dropout', 'Pre-trained MobileNetV2 (frozen features)'],\n",
    "    'Input Size': ['32x32', '224x224 (resized)'],\n",
    "    'Training Strategy': ['From scratch', 'Transfer learning']\n",
    "}\n",
    "\n",
    "# Create and display the comparison table\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"=\"*70)\n",
    "print(\"                    MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display parameter difference\n",
    "custom_params = sum(p.numel() for p in model.parameters())\n",
    "mobilenet_params = sum(p.numel() for p in mobilenet_v2.parameters())\n",
    "param_ratio = mobilenet_params / custom_params\n",
    "\n",
    "print(f\"\\nParameter Analysis:\")\n",
    "print(f\"‚Ä¢ MobileNetV2 has {param_ratio:.1f}x more parameters than Custom CNN\")\n",
    "print(f\"‚Ä¢ Custom CNN: {custom_params:,} parameters\")\n",
    "print(f\"‚Ä¢ MobileNetV2: {mobilenet_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a353a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#4B9AC7"
         },
         "mode": "lines",
         "name": "Training loss",
         "type": "scatter",
         "y": [
          1.6515597178198187,
          1.4141799517909583,
          1.306474925307057,
          1.2442186834562161,
          1.197204918812608,
          1.1517427906660778,
          1.1205082279641916,
          1.0290910384386702,
          1.0060407634433883,
          0.996773856222782,
          0.9830535658637581,
          0.9790328069568595,
          0.9743297420194387,
          0.9657411425162459,
          0.9524461258097988,
          0.9496316699039601,
          0.951577852372928,
          0.9512839385158266,
          0.9484380099474622,
          0.9492516132724255,
          0.9427748807250996,
          0.945831326420045,
          0.9373808631964047
         ]
        },
        {
         "marker": {
          "color": "#4BE8E0"
         },
         "mode": "lines",
         "name": "Validation loss",
         "type": "scatter",
         "y": [
          1.3036983726890223,
          1.1548008668194911,
          1.0997088103537347,
          0.9975559863315266,
          0.9483266886632153,
          0.9024945083697131,
          0.9398042309056421,
          0.8121567981637967,
          0.7931309196220082,
          0.7805782256612352,
          0.775288804321532,
          0.771438517957736,
          0.7642858987021598,
          0.7630425777025284,
          0.7597320239255383,
          0.7546040436644463,
          0.756578523452115,
          0.7520579893118257,
          0.7521041229272344,
          0.7569170673941351,
          0.7521344818127383,
          0.7523473394904167,
          0.7543538459547007
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training and val loss across epochs"
        },
        "xaxis": {
         "title": {
          "text": "epochs"
         }
        },
        "yaxis": {
         "title": {
          "text": "Cross Entropy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE TRAINING AND VALIDATION LOSS\n",
    "\n",
    "# Define color palette for consistent, professional visualization\n",
    "color_chart = [\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    "\n",
    "# Create interactive line plot using Plotly\n",
    "fig = go.Figure(data=[\n",
    "                      go.Scatter(\n",
    "                          y=history[\"loss\"],           # Training loss values from history\n",
    "                          name=\"Training loss\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[0]      # Blue color for training\n",
    "                          )),\n",
    "                      go.Scatter(\n",
    "                          y=history[\"val_loss\"],       # Validation loss values from history\n",
    "                          name=\"Validation loss\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[1]      # Teal color for validation\n",
    "                          ))\n",
    "])\n",
    "\n",
    "# Configure plot layout and labels\n",
    "fig.update_layout(\n",
    "    title='Training and val loss across epochs',\n",
    "    xaxis_title='epochs',\n",
    "    yaxis_title='Cross Entropy'    \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61374745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#4B9AC7"
         },
         "mode": "lines",
         "name": "Training accuracy",
         "type": "scatter",
         "y": [
          0.38608,
          0.48646,
          0.53038,
          0.55758,
          0.57516,
          0.59388,
          0.6068,
          0.63548,
          0.64608,
          0.64822,
          0.65424,
          0.65592,
          0.65864,
          0.66154,
          0.66252,
          0.66502,
          0.66294,
          0.66322,
          0.668,
          0.66402,
          0.66606,
          0.667,
          0.6693
         ]
        },
        {
         "marker": {
          "color": "#4BE8E0"
         },
         "mode": "lines",
         "name": "Validation accuracy",
         "type": "scatter",
         "y": [
          0.5236,
          0.5795,
          0.5995,
          0.6372,
          0.6645,
          0.6811,
          0.6668,
          0.7125,
          0.7166,
          0.7253,
          0.7242,
          0.7259,
          0.7283,
          0.7303,
          0.7317,
          0.7342,
          0.7322,
          0.7346,
          0.7345,
          0.7322,
          0.7331,
          0.7351,
          0.7322
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training and val accuracy across epochs"
        },
        "xaxis": {
         "title": {
          "text": "epochs"
         }
        },
        "yaxis": {
         "title": {
          "text": "Cross Entropy"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE TRAINING AND VALIDATION ACCURACY\n",
    "\n",
    "# Use same color palette for consistency\n",
    "color_chart = [\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    "\n",
    "# Create interactive line plot for accuracy metrics\n",
    "fig = go.Figure(data=[\n",
    "                      go.Scatter(\n",
    "                          y=history[\"accuracy\"],       # Training accuracy values from history\n",
    "                          name=\"Training accuracy\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[0]      # Blue for training\n",
    "                          )),\n",
    "                      go.Scatter(\n",
    "                          y=history[\"val_accuracy\"],   # Validation accuracy values from history\n",
    "                          name=\"Validation accuracy\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[1]      # Teal for validation\n",
    "                          ))\n",
    "])\n",
    "\n",
    "# Configure plot layout and labels\n",
    "fig.update_layout(\n",
    "    title='Training and val accuracy across epochs',\n",
    "    xaxis_title='epochs',\n",
    "    yaxis_title='Cross Entropy'    \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc048b7",
   "metadata": {},
   "source": [
    "THE RESULTS ARE NOT BAD ALREADY."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20e6b2",
   "metadata": {},
   "source": [
    "You can detect **overfitting** or **underfitting** by analyzing your training and validation (test) loss and accuracy curves:\n",
    "\n",
    "---\n",
    "\n",
    "### **Overfitting**\n",
    "- **Training loss** keeps decreasing (model fits training data very well).\n",
    "- **Validation loss** stops decreasing (or starts increasing), and **validation accuracy** plateaus or drops.\n",
    "- **Gap**: There is a large gap between high training accuracy and much lower validation accuracy.\n",
    "\n",
    "**What it means:**  \n",
    "Your model is memorizing the training data but not generalizing to new data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Underfitting**\n",
    "- **Both training and validation loss** are high, and **both accuracies** are low.\n",
    "- The model cannot fit the training data well.\n",
    "\n",
    "**What it means:**  \n",
    "Your model is too simple or not trained enough to capture the patterns in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Check in Your Project**\n",
    "- Look at the `history[\"loss\"]` (training loss) and `history[\"val_loss\"]` (validation loss) curves.\n",
    "- If you plot them (as you do with Plotly), check for:\n",
    "  - **Overfitting:** Training loss goes down, validation loss goes up or stagnates.\n",
    "  - **Underfitting:** Both losses stay high, and accuracy is low.\n",
    "\n",
    "---\n",
    "\n",
    "**Tip:**  \n",
    "- Overfitting: Try more regularization (dropout, weight decay), or get more data.\n",
    "- Underfitting: Try a more complex model, train longer, or reduce regularization.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "- **Big gap, low val accuracy:** Overfitting  \n",
    "- **Both low accuracy:** Underfitting  \n",
    "- **Both high accuracy, low gap:** Good fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b562ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7351\n",
      "Precision: 0.7318\n",
      "Recall: 0.7351\n",
      "F1-score: 0.7323\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# COMPREHENSIVE MODEL EVALUATION ON TEST SET\n",
    "# This function evaluates the trained model's performance using multiple metrics\n",
    "\n",
    "def test_model_evaluation():\n",
    "    # Prepare DataLoader for test set evaluation\n",
    "    test_loader = DataLoader(testset, batch_size=128, shuffle=False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = best_model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, batch norm updates)\n",
    "\n",
    "    # Initialize lists to store all predictions and true labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Perform inference without gradient computation (saves memory and speeds up evaluation)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)  # Forward pass through model\n",
    "            _, preds = torch.max(outputs, 1)  # Get predicted class indices\n",
    "            all_preds.extend(preds.cpu().numpy())  # Store predictions\n",
    "            all_labels.extend(labels.cpu().numpy())  # Store true labels\n",
    "\n",
    "    # Calculate comprehensive evaluation metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)  # Overall accuracy\n",
    "    prec = precision_score(all_labels, all_preds, average='macro')  # Macro-averaged precision\n",
    "    rec = recall_score(all_labels, all_preds, average='macro')  # Macro-averaged recall\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')  # Macro-averaged F1-score\n",
    "\n",
    "    # Display evaluation results\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "    # Generate and visualize confusion matrix to show per-class performance\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Custom CNN Confusion Matrix\")\n",
    "    \n",
    "    # Add class labels to make the confusion matrix more readable\n",
    "    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.tight_layout()  # Adjust layout to prevent label cutoff\n",
    "    plt.show()\n",
    "\n",
    "    # Sanity check: ensure model performs better than random guessing (10% for 10 classes)\n",
    "    assert acc > 0.1, \"Model accuracy is not better than random guessing.\"\n",
    "\n",
    "# Execute the evaluation function\n",
    "test_model_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa21433",
   "metadata": {},
   "source": [
    "What is a Confusion Matrix?\n",
    "A confusion matrix is a table that shows how well your model is performing on each class.\n",
    "\n",
    "Rows: Actual (true) classes\n",
    "Columns: Predicted classes\n",
    "Each cell [i, j] shows how many images of class i were predicted as class j.\n",
    "How to Read It\n",
    "Diagonal cells (top-left to bottom-right): Correct predictions for each class. Higher numbers here are good!\n",
    "Off-diagonal cells: Misclassifications. For example, if row 3, column 5 is high, your model often confuses class 3 as class 5.\n",
    "What to Look For\n",
    "Perfect classifier: Only diagonal cells are nonzero.\n",
    "If a row has many non-diagonal values: The model struggles to classify that true class.\n",
    "If a column has many non-diagonal values: The model often predicts that class incorrectly for other true classes (over-predicting).\n",
    "\n",
    "How to Use This\n",
    "Find weaknesses: Which classes are most confused? (e.g., cats vs. dogs)\n",
    "Improve model: Consider more data, augmentation, or architecture changes for confused classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6f9b07",
   "metadata": {},
   "source": [
    "Choose a Pre-trained Model\n",
    "\n",
    "Since I am looking for a pre-trained model who is efficient but ligth, I have chosen MobileNetV2. Its architecture reduces computational cost while mainting good accuracy, since it was designed for mobile. It's specifically suitable for image classification tasks of smaller images, as the CIFAR-10 dataset.  \n",
    "Although pre-trained on ImageNet (224√ó224 images), it transfers well to CIFAR-10. The learned features (edges, textures, patterns) are still relevant despite the size difference.\n",
    "\n",
    "At first, I thought about VGG, but apparently it is outdated and too heavy for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc03bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enriqueestevezalvarez/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning:\n",
      "\n",
      "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "\n",
      "/Users/enriqueestevezalvarez/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning:\n",
      "\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.8759, Acc: 0.7013, Val Loss: 0.7036, Val Acc: 0.7556, LR: 0.001000\n",
      "üíæ New best model saved! Epoch 1, Val Acc: 0.7556\n",
      "\n",
      "Training completed!\n",
      "Best model from epoch 1\n",
      "Best validation accuracy: 0.7556\n",
      "Best validation loss: 0.7036\n",
      "MobileNetV2 Accuracy: 0.7556\n",
      "MobileNetV2 Precision: 0.7683\n",
      "MobileNetV2 Recall: 0.7556\n",
      "MobileNetV2 F1-score: 0.7567\n",
      "MobileNetV2 Accuracy: 0.7556\n",
      "MobileNetV2 Precision: 0.7683\n",
      "MobileNetV2 Recall: 0.7556\n",
      "MobileNetV2 F1-score: 0.7567\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmIAAAIjCAYAAACgUncvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABDg0lEQVR4nO3dC5xM9f/48ffsYndddl3KLetSCRtyK0nfbkQl0V1fakO6uSviW+6XjQpfFFFJRaWLkkqJIiF3f7n35RsR6hu7Ll9r7c7/8f70nfntWbvMMnPOnpnXs8dpd86cOfOZs7Pmve/3+3yOx+v1egUAAAC2i7L/KQEAAKAIxAAAABxCIAYAAOAQAjEAAACHEIgBAAA4hEAMAADAIQRiAAAADiEQAwAAcAiBGAAAgEMIxBBWPB6PdOvW7azbvfnmm2bbf//73/51N9xwg1lQMO3YsUNatGghCQkJ5mf3ySefBHX/+l7Q/ep7A3/hdwIIPQIx2MYX/OiydOnS0+7Xq20lJiaa+2+//XZxk6pVq5pxd+/e/bT7vvvuO3Pfhx9+mO/97tu3T4YMGSLr16/3r8vIyJALLrhArr322jwf5zuWDRo0MLe3bt0q/fr1k3r16kmJEiWkQoUK0qpVK1m9enW+xvOvf/1LHnvsMbn44oslNjZW4uPjpWnTpvLPf/5T/vvf/0ooJScny8aNG2XkyJHy9ttvS6NGjSRcPPzww+Y9osczt+OoQajvd+fFF18MyvsIQMFAIAbb6Qf4rFmzTlu/ePFi+fXXXyUmJibkY3jwwQfNB16VKlWCut9p06aZD71g0X0NHTrU8gFauHBhuffee2XZsmXyyy+/5Pq4JUuWmGPZoUMHc/u1114zY9Pg5aWXXpI+ffrItm3b5Oqrr5ZvvvkmoLF8/vnnUqdOHZk9e7a0bt1aJk6cKCkpKVK5cmXp27ev9OzZU0JFf1bLly+Xzp07m4ynvq5KlSoF9Tn0vaDPo+8NJxQqVEiOHz8un3322Wn3zZw50/zeBPN9FIivv/7aLABCh0AMtrvtttvkgw8+kFOnTlnWa3DWsGFDKV++fMjHEB0dbT7YNMMQLJdffrlkZmbK888/L6HWvn17k/V69913c71fj2VUVJS0a9fO3H7ggQdkz549JiB79NFHTeD0448/SunSpU2m5Gx27dpl9qXByubNm00GrEuXLtK1a1czBl2nrz9Ufv/9d/O1ZMmSIXsOfS/oe0LfG07QP0CaNWuW689Uf56awbSLBoSqSJEiZgEQOgRisJ0GBf/5z39kwYIF/nUnT540pbu///3vuT7m2LFj8tRTT5lym35g1ahRw5RoNBjJjWYQdBv9YNXgTjNEZ+sRy016eroMHjxYLr30UvO8+vxa4tP1uZUnH3rooYCzYnv37pVOnTpJuXLlzL41kHnjjTcsJc0rr7zSfN+xY0d/aUrHruVAfb7cMotautRjeeONN0rFihXNOj0GxYsXt2xXpkwZ+dvf/iZbtmw561jHjBkjR48elddff92UNXPS45M9I6ZB9vDhw+WSSy4xr03H+o9//OO046brtQytpeqrrrrK/Ly07PnWW2/5t9FA0Ze51ABSj4E+zlfS832fnT4mZ5Ct7zct52owp8dC3x86prP1iC1atMgcp2LFipnHtmnT5rRj5nu+n3/+2YxJt9NeNv25+YKaQOj7/8svv5TDhw/7161atcqUJnP73fjzzz/l6aefNplKfU1a2rz11ltlw4YNAb2PlPaA1a5dW9asWSPXXXedFC1a1H9ccvaIaXlYf0Y5X3/Lli2lVKlSQc0GA5GCQAy20w/OJk2aWP7y1w+f1NRUfwYnOw227rjjDhk3bpzccsstMnbsWPMhqh/KWmLLrcTZq1cvU74aNmyYCfr0cT/99FO+xpmVlWWeVwM+Xymubdu2Zhz3339/ro959tlnTRBytqzYgQMH/GVBLbVphkmDGS29jR8/3mxTq1YtM36lWSzti9JFPyz1g1Q/mLVnatOmTZZ9z58/33xAa9bsbPbv32/6zc5Gy2UaIF1zzTUSiEceeUQGDRpketT0eF1//fWmjJnbz1eDl3vuuUduvvlmUzbVD3QNZnyv66677jL78AXxegx8xyhQui8N+DQQ1GOqz6M/2x9++OGMj9OfjwYZBw8eNMGWvt+0JKyBcG5B/H333SdHjhwxr1W/12BHS4KB0teqP9uPP/7Yv06D7Zo1a/r7/bLbuXOnOWlBX5v+XujvhL4n9Hj7gqIzvY989HdEAzjtIdRjq0F8bvR9euGFF5qATLO/6tVXXzXlS/398AX+APLBC9hk+vTpmr7yrlq1yjtp0iRviRIlvMePHzf33Xvvvd4bb7zRfF+lShVvq1at/I/75JNPzONGjBhh2d8999zj9Xg83p9//tm/TrfTZfXq1f51v/zyizc2NtZ75513njaWXbt2+dddf/31ZvF5++23vVFRUd7vv//e8rxTpkwxj/3hhx/867KPuWPHjub59u3bZ25/++23ZvsPPvjAv33nzp29FSpU8P7xxx+Wfbdr186bkJDgPy56rPSxOt6cNm3aZO4bMGDAafvQ509NTfWeyZIlS8zxGzhw4Bm30/3o87Rp08YbiPXr15vtH3nkEcv6p59+2qxftGiR5bjpOh2Lz8GDB70xMTHep556yr9Of0663QsvvGDZZ3JystlHToMHDzbb+4wbN87c/v333/Mct+85sh/revXqecuWLev9z3/+41+3YcMG87546KGHTnu+Tp06Wfap77kyZcrk+ZzZX0exYsX87+tmzZqZ7zMzM73ly5f3Dh06NNdjcOLECbNNztehx2/YsGH+dWd6H+l7Xu/T93Vu92X/nVBfffWV//dx586d3uLFi3vbtm171tcIIHdkxOAIzRZoY/S8efNMBkG/5lWW/OKLL0zfTo8ePSzrtVSpsZdm07LTbJuW4ny0mVzLSV999ZX/r/hAaB+bZhM0G/HHH3/4l5tuusnc/+233+b6uOeee+6MWTEd80cffWSybPp99n1r9kUzg2vXrj3r+JKSkqR+/fry3nvvWUq4c+fONRkSLVPlRTM8eryrVatmSq1nkpaWZr7q2ZaB0J+Xypmt1J+Xr+k/5+vQ0p+PZlw046nZnmDx9ZZ9+umnJtMZiN9++800t2t2TnvpfOrWrWuyd77Xmd3jjz9uua2vS7NNvmMYCP25aDlRs5VaFtWvef1uaNlXewGVvrf1uXxl10DeQ9n3o2XLQOgUInrmrGbZNIOnpUrNigE4NwRicIR+2DZv3tyUXbQMox8iWp7KjZ4ZqCWPnIGABkm++7OrXr36afu47LLLTK+Or+k7ENqXoyUtHWv2RfflC2ZyoyU8PfNu6tSp5sM8Jx2D9gDp/Tn37fswzGvfOWn5URvptVymtEylr/NMZUkN1jRQ0wBYA5OcvWM5+QI63T4Q+vPQ4EBLrdnpSRgaEOX8eWmgnJOWJw8dOiTBoqVkLSdqyVR78rREqmd/niko841Tg5qc9L2ngbMeyzO9Fn0dKj+vRU9m0ff6+++/b3odtb8r57H00fFr2Vbf8xpMaZlZ30f/7//9PxPQB+qiiy7KV1O+lus1ONVAdcKECVK2bNmAHwvAqlCO24Bt9K98PfNO/+LX/pRQnhF3LvRDTpugtfcmN9q4nxftFdM+nNGjR5u+spz7VdrDpr02udGsSyC0Z0ozWhrQav+WftUPf/0wz42eFKFZDP2g1gyhNmmfjQZiGgjnt8cu0DNS8zpLMa8TMQJ5jpyZz7i4OHPChmYxNSOnfXQa6Gh2U/ubgnWm5Pm8Fh8NqPRnNGPGDJMVPNNZraNGjZKBAweakz705AgNjjQI1h7JQDN/vuOTH+vWrfP/saA9afo+BHBuCMTgmDvvvNOUOFasWGE+FPOiZ8xp07RmZLJnxXSSUt/9OTNZOW3fvt2cDabZgkDpGX969plOKZDfaS70sRpoacmmcePGlvt0DPo6NFjQrOCZnO15NUDSxmoto+oHsp4ZqKW03LIb+sGsZ3UuXLjQZIO0oTtQmkHTDJ7O5aWl3zPRn4c+l/4cfFlL3wkKmgkM5txtGnRmP8PQJ7f51TRA0Z+lLhpcaxCjAbMGZ7n9HHzj1PnWctL3nmaf9EzKUP2RomfQZp+CJDe+s2P1bNbs9JhkPwkjmNO0aBZQM7daUtbgX8+o1d9l35mZAPKH0iQcoyWxyZMnm7/4tV8qL5rd0aBl0qRJlvVaktEPGM2mZafBQvb+GJ0/S0tw2tuSn8yH9rHpFBM6HUVO2t+WsyyVW6+YTiWhH1TZ6Rjuvvtu0yeWW5Ype/nU90GfW7Dho2VIzU5oUKvPl1dZUmf914D3lVdeMRmX/NCsm45FS3saUOU2476eUad82bicZzb6MovBnA9LA14twWmGz0fLwXPmzLFsp2eR5qRnCKrcpiJROk2HbqOZqezHX39mmkXLK+sYDBpcaYZL3/NnmldP30s5s20alOv7NrtA3keBeuaZZ2T37t3muOjPVM+C1sxuXscRwJmREYOj8irNZadBmn4wafZCpwy44oorzAehBldagtEP4+y03KZN79rcr2UeDTxUfqYRUNrnpZkjbcDWrIn2GGlAqNkQXa+lvTNdZseXFdMPrJy0kV/3qdkyLc9qdkGDBQ0gNfvnCxx0H1qynTJlismi6QeqPkab7H00qHvyySfN8dByafZpCXw0KNLjoNkszQy+8847lvs1o3Gm7I6OQ8ue2mulWS7NrOlx1lKn9qfph79m4pT+fPTnqhk0/eDXzNvKlSvNcdAybV5TI5wLzRZpYKDj15+39sdpcK99fNmDcW0s19KkBoGa6dLAVY+Hzs5/pktFvfDCCybQ1+OmU4toAK7TNOgcYYFMhHuuNBOmgXwgmUp9bZqh0uyUlgm1r0z7FLML5H0UCD15QI+bzq3nm05j+vTpZq4xzcjm/KMDQADyOJsSCOn0FWeSc/oKdeTIEW/v3r29FStW9BYuXNhbvXp1cxp/VlaWZTvdf9euXb3vvPOO2UZP469fv76ZQiK3sZxp+gp18uRJ7+jRo72XX3652VepUqW8DRs2NNMJZJ8eIrcxqx07dnijo6NPm75CHThwwIw1MTHRvCadpkCnLZg6daplu08//dSblJTkLVSoUJ5TEOj0H3pfv3798pwewTe1R25L9uNwJtu3b/d26dLFW7VqVW+RIkXMFCRNmzb1Tpw40Uyl4JORkWGOUbVq1cxr09eo02xk3+ZMxy3nzyKv6SvU119/7a1du7YZT40aNczPPuf0FQsXLjTTb+j7R7fTrw888IB5PTmfI+fx/eabb8xrjIuL88bHx3tbt27t3bx5s2Ub3/PlnB4jt/fZ2aavyEte01foNB86FYqOT8e5fPnyXN/Leb2PdDt9f+cm+37S0tLMz6tBgwbm55ud/m7qlB763ADyx6P/CyRgAwAAQHDRIwYAAOAQAjEAAACHEIgBAAA4hEAMAADAIQRiAAAADiEQAwAAcIirJ3TVy6js27fPTFAYzEt4AAAQjnTGKr1cnF4eTScOttuJEyfMRNChUKRIEYmNjRW3cXUgpkHYmS68DAAATqeXftMrS9gdhMWVKCNy6nhI9l++fHnZtWuX64IxVwdivgtAF7tjnHgKx4mbbJx4r7iSi6f/LVLYnZX4KJdme106bFdz6/TcJzOzxK0KRbnrjX7kSJrUvKSK//PTTiYTduq4xCQli0QXCe7OM0/K/s0zzHMQiNnIV47UIMxtgViJ+HhxJZf+Q69iCMRs5dJhuxqBmP3cFoj5ONrOUyhWPEEOxLwed/777vpADAAAuIzGgMEOBD3iWu4NIQEAAFyOjBgAALCPlhGDXUr0uDev5N6RAwAAuBwZMQAAYB/tDwt6j5hH3IqMGAAAgEPIiAEAAPvQI2bh3pEDAAC4HBkxAABgH3rELAjEAACAjUJQmhT3FvjcO3IAAACXIyMGAADsQ2nSgowYAACAQ8iIAQAA+zB9hYV7Rw4AAOByZMQAAIB96BEreBmxl19+WapWrSqxsbHSuHFjWblypdNDAgAACP9A7P3335c+ffrI4MGDZe3atXLFFVdIy5Yt5eDBg04PDQAAhKpHLNiLSzk+8rFjx0qXLl2kY8eOkpSUJFOmTJGiRYvKG2+84fTQAABAqEqTwV5cytFA7OTJk7JmzRpp3rz5/w0oKsrcXr58+Wnbp6enS1pammUBAABwK0cDsT/++EMyMzOlXLlylvV6e//+/adtn5KSIgkJCf4lMTHRxtECAIDzRmnSwlUjHzBggKSmpvqXPXv2OD0kAAAAd05fccEFF0h0dLQcOHDAsl5vly9f/rTtY2JizAIAAFzK9HQFe0JXj7iVoxmxIkWKSMOGDWXhwoX+dVlZWeZ2kyZNnBwaAABA+E/oqlNXJCcnS6NGjeSqq66S8ePHy7Fjx8xZlAAAIMxEef5agr1Pl3I8ELv//vvl999/l0GDBpkG/Xr16sn8+fNPa+AHAAAIN44HYqpbt25mAQAAYY6Lfhe8QAwAAEQIrjVp4d4QEgAAwOXIiAEAAPtQmrRw78gBAABcjowYAACwDz1iFmTEAAAAHEJGDAAA2IceMQv3jhwAAMDlyIgBAAD70CNmQSAGAADsQ2nSwr0jBwAAcDkyYgAAwD6UJi3IiAEAADiEjBgAALBRCHrExL15JfeOHAAAwOXCIiP286vtJD4+Xtyk9FXdxY0OrZokbpWZ5RU3iopyZ++D1+vO4+1mbn2vxEZFi1tlnMoSNykQv5b0iFmQEQMAAHBIWGTEAACAS5iMWLDnEfOIWxGIAQAA+zChq4V7Rw4AAOByZMQAAIB9aNa3ICMGAADgEDJiAADAPvSIWbh35AAAAC5HRgwAANiHHjELMmIAAAAOISMGAADsQ4+YBYEYAACwD6VJC/eGkAAAAC5HRgwAANjG4/GYJcg7FbciIwYAAOAQMmIAAMA2ZMSsyIgBAAA4hIwYAACwjyavgp3A8ohrkREDAABwCBkxAABgG3rErMiIAQAA2wOxYC/5kZmZKQMHDpRq1apJXFycXHLJJTJ8+HDxer3+bfT7QYMGSYUKFcw2zZs3lx07dlj28+eff0r79u0lPj5eSpYsKZ07d5ajR4/maywEYgAAIKKMHj1aJk+eLJMmTZItW7aY22PGjJGJEyf6t9HbEyZMkClTpsiPP/4oxYoVk5YtW8qJEyf822gQtmnTJlmwYIHMmzdPlixZIo8++mi+xkJpEgAARFRpctmyZdKmTRtp1aqVuV21alV59913ZeXKlf5s2Pjx4+W5554z26m33npLypUrJ5988om0a9fOBHDz58+XVatWSaNGjcw2Gsjddttt8uKLL0rFihUDGgsZMQAAEBbS0tIsS3p6eq7bXXPNNbJw4ULZvn27ub1hwwZZunSp3Hrrreb2rl27ZP/+/aYc6ZOQkCCNGzeW5cuXm9v6VcuRviBM6fZRUVEmgxYoMmIAACAsMmKJiYmW1YMHD5YhQ4actnn//v1NoFazZk2Jjo42PWMjR440pUalQZjSDFh2ett3n34tW7as5f5ChQpJ6dKl/dsEgkAMAACEhT179pjGeZ+YmJhct5s9e7bMnDlTZs2aJZdffrmsX79eevXqZcqJycnJNo6YQAwAAITJhK7x8fGWQCwvffv2NVkx7fVSderUkV9++UVSUlJMIFa+fHmz/sCBA+asSR+9Xa9ePfO9bnPw4EHLfk+dOmXOpPQ9PhD0iAEAgIhy/Phx08uVnZYos7KyzPc6rYUGU9pH5qOlTO39atKkibmtXw8fPixr1qzxb7No0SKzD+0lCxQZMQAAEFFnTbZu3dr0hFWuXNmUJtetWydjx46VTp06+ceopcoRI0ZI9erVTWCm845p6bJt27Zmm1q1asktt9wiXbp0MVNcZGRkSLdu3UyWLdAzJhWBGAAAiCgTJ040gdWTTz5pyosaOD322GNmAleffv36ybFjx8y8YJr5uvbaa810FbGxsf5ttM9Mg69mzZqZDNvdd99t5h7LD483+zSyLqNpQj2ddP8fhwOqCRckpa/qLm50aNUkcavMLHe+1aOj3HnpDhf/0+JaQc8y4KwyTv1VynLT52alcqUkNTXV9s9N32d2wn1TxVO4aFD37c04LqmzH3XkdZ0vMmIAAMA2Hv0v6H80eMStaNYHAABwCBkxAAAQUc36BQkZMQAAAIeQEQMAAGExoasbkREDAABwCBkxAABgnxD0iHnpEQMAAEB+kREDAACuPmvS4+KMGIEYAACwDYGYFaVJAAAAh5ARAwAA9mH6CgsyYgAAAA4hIwYAAGxDj5gVGTEAAACHhEVGLD0jyyxu8p8fJ4ob1ejzmbjV6pG3iBtFRbn0Lz2v0wOIPEVjop0eQsQpXMhd+YyCMF4yYlbO/0QAAAAiVFhkxAAAgDuQEbMiEAMAALYhELOiNAkAAOAQMmIAAMA+TOhqQUYMAADAIWTEAACAbegRsyIjBgAA4BAyYgAAwDZkxKzIiAEAADiEjBgAALANGTErAjEAAGAfpq+woDQJAADgEDJiAADANpQmrciIAQAAOISMGAAAsA0ZMSsyYgAAAA4hIwYAAGzjkRBkxISM2DlJSUmRK6+8UkqUKCFly5aVtm3byrZt25wcEgAAQGQEYosXL5auXbvKihUrZMGCBZKRkSEtWrSQY8eOOTksAAAQ4h6xYC9u5Whpcv78+Zbbb775psmMrVmzRq677jrHxgUAAEKECV0Lbo9Yamqq+Vq6dOlc709PTzeLT1pamm1jAwAACNuzJrOysqRXr17StGlTqV27dp49ZQkJCf4lMTHR9nECAIBzR2mygAZi2iv2008/yXvvvZfnNgMGDDBZM9+yZ88eW8cIAAAQdqXJbt26ybx582TJkiVSqVKlPLeLiYkxCwAAcCcmdC1AgZjX65Xu3bvLnDlz5LvvvpNq1ao5ORwAAIDICcS0HDlr1iz59NNPzVxi+/fvN+u1/ysuLs7JoQEAgBDQ5FWwE1ge9ybEnO0Rmzx5sun1uuGGG6RChQr+5f3333dyWAAAAJFRmgQAAJGWEQt2j5i4VoFo1gcAABEiBKVJcXEgVmCmrwAAAIg0ZMQAAIBtmL7CiowYAACAQ8iIAQAA2zB9hRUZMQAAAIeQEQMAALaJivKYJZi8Qd6fnciIAQAAOISMGAAAsA09YlYEYgAAwDZMX2FFaRIAAMAhZMQAAIBtKE1akREDAABwCBkxAABgG3rErMiIAQAAOISMGAAAsA0ZsTAMxAoXijKLm3jFndam3Cpu1fzFJeJG3/W7Xtxo36ET4lYXxseIG2VmufNflmgXz4ru9brrmLttvJEgLAIxAADgDpw1aUUgBgAAbOOREJQmxb2RmLvqeQAAAGGEjBgAALANpUkrMmIAAAAOISMGAABsw/QVVmTEAAAAHEJGDAAA2IYeMSsyYgAAAA4hIwYAAGxDj5gVGTEAAACHkBEDAAC2oUfMikAMAADYhtKkFaVJAAAAh5ARAwAA9glBaVLcmxAjIwYAAOAUMmIAAMA29IhZkREDAABwCBkxAABgG6avsCIjBgAA4BAyYgAAwDb0iFkRiAEAANtQmrSiNAkAAOAQMmIAAMA2lCatyIgBAAA4hEAMAADYnhEL9pJfe/fulQ4dOkiZMmUkLi5O6tSpI6tXr/bf7/V6ZdCgQVKhQgVzf/PmzWXHjh2Wffz555/Svn17iY+Pl5IlS0rnzp3l6NGj+RoHgRgAAIgohw4dkqZNm0rhwoXlyy+/lM2bN8tLL70kpUqV8m8zZswYmTBhgkyZMkV+/PFHKVasmLRs2VJOnDjh30aDsE2bNsmCBQtk3rx5smTJEnn00UfzNRZ6xAAAQESdNTl69GhJTEyU6dOn+9dVq1bNkg0bP368PPfcc9KmTRuz7q233pJy5crJJ598Iu3atZMtW7bI/PnzZdWqVdKoUSOzzcSJE+W2226TF198USpWrBjQWMiIAQCAsJCWlmZZ0tPTc91u7ty5Jni69957pWzZslK/fn2ZNm2a//5du3bJ/v37TTnSJyEhQRo3bizLly83t/WrliN9QZjS7aOiokwGLVAEYgAAICx6xBITE03A5FtSUlJyHcPOnTtl8uTJUr16dfnqq6/kiSeekB49esiMGTPM/RqEKc2AZae3fffpVw3isitUqJCULl3av00gKE0CAICwKE3u2bPHNM77xMTE5Lp9VlaWyWSNGjXK3NaM2E8//WT6wZKTk8VOZMQAAEBYiI+Ptyx5BWJ6JmRSUpJlXa1atWT37t3m+/Lly5uvBw4csGyjt3336deDBw9a7j916pQ5k9K3TSAIxAAAQERNX9G0aVPZtm2bZd327dulSpUq/sZ9DaYWLlzov197zrT3q0mTJua2fj18+LCsWbPGv82iRYtMtk17yQJFaRIAAESU3r17yzXXXGNKk/fdd5+sXLlSpk6dahalgV2vXr1kxIgRpo9MA7OBAweaMyHbtm3rz6Ddcsst0qVLF1PSzMjIkG7dupkzKgM9Y1IRiAEAANto7iroPWKSP1deeaXMmTNHBgwYIMOGDTOBlk5XofOC+fTr10+OHTtm5gXTzNe1115rpquIjY31bzNz5kwTfDVr1sycLXn33Xebucfyg0AMAABEnNtvv90sedGsmAZpuuRFz5CcNWvWeY2DQAwAANgmyuMxS7D36VY06wMAADiEjBgAAIioSxwVJARiAADANucy3cTZBHt/dqI0CQAA4BAyYgAAwDZRnr+WYO/TrciIAQAAOISMGAAAsI9p1nd4RtcChIwYAACAQ8iIOSTapQXtEyczxa3m9/6buNHVw74RN1o5+GZxq/9muPN9nn4qS9yoWAwfRXYpCGcXMn2FFRkxAAAAh/BnCAAAsI3nf/8Fe59uRSAGAABsw/QVVpQmAQAAHEJGDAAA2IZLHFmREQMAAHAIGTEAAGAbpq+wIiMGAADgEDJiAADANlEej1mCvU+3IiMGAADgEDJiAADANvSIWRGIAQAA2zB9hRWlSQAAAIeQEQMAALahNHkOgdjcuXMlUHfccUfA2wIAAESygAKxtm3bBlyjzczMPN8xAQCAMMX0FecQiGVlZQWyGQAAAOxq1j9x4sT5PBwAAEQYT4iWiAnEtPQ4fPhwueiii6R48eKyc+dOs37gwIHy+uuvh2KMAAAAYSnfgdjIkSPlzTfflDFjxkiRIkX862vXri2vvfbaOQ/k+eefNz1mvXr1Oud9AAAAd8wjFuwlYgKxt956S6ZOnSrt27eX6Oho//orrrhCtm7dek6DWLVqlbz66qtSt27dc3o8AABwhyhPaJaICcT27t0rl156aa4N/RkZGfkewNGjR01QN23aNClVqlS+Hw8AABAxgVhSUpJ8//33p63/8MMPpX79+vkeQNeuXaVVq1bSvHnzs26bnp4uaWlplgUAALgHpcnznFl/0KBBkpycbDJjmgX7+OOPZdu2baZkOW/evHzt67333pO1a9ea0mQgUlJSZOjQofkdMgAAQHhkxNq0aSOfffaZfPPNN1KsWDETmG3ZssWsu/nmmwPez549e6Rnz54yc+ZMiY2NDegxAwYMkNTUVP+i+wAAAO68zJEnSEvEXWvyb3/7myxYsOC8nnjNmjVy8OBBadCggWVqjCVLlsikSZNMGTL7yQAqJibGLAAAABF90e/Vq1ebTJivb6xhw4b5enyzZs1k48aNlnUdO3aUmjVryjPPPHNaEAYAANwvFD1dHhenxfIdiP3666/ywAMPyA8//CAlS5Y06w4fPizXXHON6fmqVKlSQPspUaKEmXssOy11lilT5rT1AAAA4SjfPWKPPPKImaZCs2F//vmnWfR7bdzX+wAAAPLCPGLnmRFbvHixLFu2TGrUqOFfp99PnDjR9I6dj+++++68Hg8AAAo2SpPnmRFLTEzMdeJWbbSvWLFifncHAAAQsfIdiL3wwgvSvXt306zvo9/rVBQvvvhisMcHAADCiCdES1iXJvXSQ9nTfseOHZPGjRtLoUJ/PfzUqVPm+06dOknbtm1DN1oAAIAwElAgNn78+NCPBAAAhL0oj8cswd5nWAdiekkjAAAAFJAJXdWJEyfk5MmTlnXx8fHnOyYAABCmQnFZIo8ngpr1tT+sW7duUrZsWTMBq/aPZV8AAAAQokCsX79+smjRIpk8ebK57uNrr70mQ4cONVNXvPXWW/ndHQAAiMB5xIK9uFW+S5OfffaZCbhuuOEGc21IncT10ksvlSpVqsjMmTOlffv2oRkpAABApGfE9JJGF198sb8fTG+ra6+9VpYsWRL8EQIAgLDrEQv2EjGBmAZhu3btMt/XrFlTZs+e7c+U+S4CDgAAcKbpK4K9REwgpuXIDRs2mO/79+8vL7/8ssTGxkrv3r2lb9++oRgjAABAWMp3j5gGXD7NmzeXrVu3ypo1a0yfWN26dYM9PgAAEEaYviKI84gpbdLXBQAAACEIxCZMmBDwDnv06JHPIQAAgEgRiukmPC5OiQUUiI0bNy7gA+FEIBbl+Wtxk8wsr7hRibjC4lYHUk+IG/04+GZxo+rdPxa3+nnSXU4PAUCECCgQ850lCQAAcL5nCUaFYJ9u5eaxAwAAuNp5N+sDAAAEih4xKwIxAABgG08I+ro97o3DKE0CAAA4hYwYAABw9UwHUZGWEfv++++lQ4cO0qRJE9m7d69Z9/bbb8vSpUuDPT4AAICwle9A7KOPPpKWLVtKXFycrFu3TtLT08361NRUGTVqVCjGCAAAwqxZP9hLxARiI0aMkClTpsi0adOkcOH/m9yzadOmsnbt2mCPDwAAIGzlu0ds27Ztct111522PiEhQQ4fPhyscQEAgDBEj9h5ZsTKly8vP//882nrtT/s4osvzu/uAAAAIla+A7EuXbpIz5495ccffzQ12X379snMmTPl6aeflieeeCI0owQAAGFB27lCsURMabJ///6SlZUlzZo1k+PHj5syZUxMjAnEunfvHppRAgCAsBDl8Zgl2PuMmEBMs2DPPvus9O3b15Qojx49KklJSVK8ePHQjBAAACBMnfOErkWKFDEBGAAAQH56ooJ9WZ8oiaBA7MYbbzzjfB2LFi063zEBAABEhHwHYvXq1bPczsjIkPXr18tPP/0kycnJwRwbAAAIM6Forvd4IigQGzduXK7rhwwZYvrFAAAAYHNZVa89+cYbbwRrdwAAIAxFyV9nTQZ1EfemxIIWiC1fvlxiY2ODtTsAAICwl+/S5F133WW57fV65bfffpPVq1fLwIEDgzk2AAAQZugRO89ATK8pmV1UVJTUqFFDhg0bJi1atMjv7gAAQAThWpPnEYhlZmZKx44dpU6dOlKqVKn8PBQAAADn0yMWHR1tsl6HDx/Oz8MAAAD8ZcRgN+t7XJwRy3ezfu3atWXnzp2hGQ0AAEAEyXcgNmLECHOB73nz5pkm/bS0NMsCAABwtmb9YC9h3yOmzfhPPfWU3Hbbbeb2HXfcYbnUkZ49qbe1jwwAAABBDMSGDh0qjz/+uHz77beBPgQAAMCCsybPMRDTjJe6/vrrA30IAAAAgjV9RfZSJAAAQH55/vdfsPcZEYHYZZdddtZg7M8//zzfMQEAgDBFafI8AjHtE8s5sz4AAICbPf/88zJgwADp2bOnjB8/3qw7ceKEOUnxvffek/T0dGnZsqW88sorUq5cOf/jdu/eLU888YTpny9evLgkJydLSkqKFCpUKDSBWLt27aRs2bL5eQgAAECBzYitWrVKXn31Valbt65lfe/eveXzzz+XDz74wCShunXrZq63/cMPP5j7dZaIVq1aSfny5WXZsmVmSq+HHnpIChcuLKNGjQp87IFuSH8YAAAIJ0ePHpX27dvLtGnTLJduTE1Nlddff13Gjh0rN910kzRs2FCmT59uAq4VK1aYbb7++mvZvHmzvPPOO1KvXj259dZbZfjw4fLyyy/LyZMngx+I+c6aBAAAOFea2AnFonJOMq8lxTPp2rWryWo1b97csn7NmjWSkZFhWV+zZk2pXLmyLF++3NzWr3rt7eylSi1f6vNu2rRJgl6azMrKCninAAAAdktMTLTcHjx4sAwZMiTXbbX3a+3ataY0mdP+/fulSJEiUrJkSct6Dbr0Pt822YMw3/2++0LSIwYAAFBQe8T27Nkj8fHx/vUxMTG5bq/baWP+ggULJDY2Vlx1rUkAAICCKD4+3rLkFYhp6fHgwYPSoEEDc4ajLosXL5YJEyaY7zWzpX1ehw8ftjzuwIEDpjlf6Ve9nfN+332BIhADAAARddHvZs2aycaNG2X9+vX+pVGjRqZx3/e9nv24cOFC/2O2bdtmpqto0qSJua1fdR8a0Plohk0DwKSkpIDHQmkSAADYJsrjMUuw95kfJUqUkNq1a1vWFStWTMqUKeNf37lzZ+nTp4+ULl3aBFfdu3c3wdfVV19t7m/RooUJuB588EEZM2aM6Qt77rnnzAkAeWXickMgBgAAkMO4ceMkKipK7r77bsuErj7R0dEyb948M6GrBmgayOmErsOGDZP8IBADAAARO6Grz3fffSfZaRO/zgmmS16qVKkiX3zxhZwPesQAAAAcQkYMAADY5xya68/KxRf/ISMGAADgEDJiAADANlHiMUuw9+lWBGIOiQ52p6JN/nsyU9yqdPEi4kYufavIjol3iVtdeN9UcaNDHz3u9BAA5BOBGAAAsM25TMB6NkHvObMRgRgAAJBIn77CKTTrAwAAOISMGAAAiKhLHBUkZMQAAAAcQkYMAADYhmZ9KzJiAAAADiEjBgAA7J3QNdg9YuLelBgZMQAAAIeQEQMAALahR8yKQAwAANhaigt2OS5K3MvNYwcAAHA1MmIAAMA2Ho/HLMHep1uREQMAAHAIGTEAAGAbzV0FO3/lEfciIwYAAOAQMmIAAMA2XPTbiowYAABApAZie/fulQ4dOkiZMmUkLi5O6tSpI6tXr3Z6WAAAIMR9Yp4gLW7maGny0KFD0rRpU7nxxhvlyy+/lAsvvFB27NghpUqVcnJYAAAgRJhZvwAFYqNHj5bExESZPn26f121atWcHBIAAEBklCbnzp0rjRo1knvvvVfKli0r9evXl2nTpuW5fXp6uqSlpVkWAADgvgldg724laOB2M6dO2Xy5MlSvXp1+eqrr+SJJ56QHj16yIwZM3LdPiUlRRISEvyLZtMAAADcytFALCsrSxo0aCCjRo0y2bBHH31UunTpIlOmTMl1+wEDBkhqaqp/2bNnj+1jBgAA53/R72AvbuXo2CtUqCBJSUmWdbVq1ZLdu3fnun1MTIzEx8dbFgAAALdytFlfz5jctm2bZd327dulSpUqjo0JAACEDhf9LkAZsd69e8uKFStMafLnn3+WWbNmydSpU6Vr165ODgsAACD8A7Err7xS5syZI++++67Url1bhg8fLuPHj5f27ds7OSwAAOCSyVw9Lp/U1fFrTd5+++1mAQAAiDSOB2IAACBy0CNmRSAGAABsE4rpJqLEvdw8dgAAAFcjIwYAAGxDadKKjBgAAIBDyIgBAADbhGK6CY+4FxkxAAAAh5ARAwAAttF2rmC3dHlcnBIjIwYAAOAQMmIAAMA2UeIxS7D36VYEYgAAwDaUJq0oTQIAADiEjBgAALCN53//BXufbkVGDAAAwCFkxAAAgG3oEbMiIwYAAOCQsMiIZWZ5zeImbo3eC0V73Ps+yXTXe8TnWHqm00OIOP/54DFxo8t6zxU32j7uDnErr9dd/64UhPFqP1ewp5vw0CMGAACAiMyIAQAAd6BHzIpADAAA2IZAzIrSJAAAgEPIiAEAANswoasVGTEAAACHkBEDAAC2ifL8tQR7n25FRgwAAMAhZMQAAIBt6BGzIiMGAADgEDJiAADANswjZkUgBgAAbKMxU/BLk+5FaRIAAMAhZMQAAIBtmL7CiowYAACAQ8iIAQAA2zB9hRUZMQAAAIeQEQMAALZh+gorMmIAAAAOISMGAABsnkcs+Pt0KwIxAABgmyjxSFSQa4lRLg7FKE0CAAA4hIwYAACwDaVJKzJiAAAADiEjBgAA7ENKzIKMGAAAgEPIiAEAANtwiSMrMmIAAAAOISMGAADsE4JLHIl7E2IEYgAAwD706ltRmgQAAHAIGTEAAGAfUmIWZMQAAAAcQkYMAADYhukrrMiIAQAAOISMGAAAsI0nBNNXeNybECMjBgAAIktKSopceeWVUqJECSlbtqy0bdtWtm3bZtnmxIkT0rVrVylTpowUL15c7r77bjlw4IBlm927d0urVq2kaNGiZj99+/aVU6dO5WssBGIAAMD2kyaDveTH4sWLTZC1YsUKWbBggWRkZEiLFi3k2LFj/m169+4tn332mXzwwQdm+3379sldd93lvz8zM9MEYSdPnpRly5bJjBkz5M0335RBgwblayyUJgEAQERNXzF//nzLbQ2gNKO1Zs0aue666yQ1NVVef/11mTVrltx0001mm+nTp0utWrVM8Hb11VfL119/LZs3b5ZvvvlGypUrJ/Xq1ZPhw4fLM888I0OGDJEiRYoENBYyYgAAICykpaVZlvT09IAep4GXKl26tPmqAZlmyZo3b+7fpmbNmlK5cmVZvny5ua1f69SpY4Iwn5YtW5rn3bRpU8BjJhADAAC2T18R7P9UYmKiJCQk+BftBTubrKws6dWrlzRt2lRq165t1u3fv99ktEqWLGnZVoMuvc+3TfYgzHe/775AUZoEAABhYc+ePRIfH++/HRMTc9bHaK/YTz/9JEuXLhUnEIgBAICwmL4iPj7eEoidTbdu3WTevHmyZMkSqVSpkn99+fLlTRP+4cOHLVkxPWtS7/Nts3LlSsv+fGdV+rYJBKVJAAAQUbxerwnC5syZI4sWLZJq1apZ7m/YsKEULlxYFi5c6F+n01vodBVNmjQxt/Xrxo0b5eDBg/5t9AxMDQSTkpICHgsZMQAAEEknTYqWI/WMyE8//dTMJebr6dK+sri4OPO1c+fO0qdPH9PAr8FV9+7dTfClZ0wqne5CA64HH3xQxowZY/bx3HPPmX0HUhINq0Ds9yPpckICOzOioIgrEi1uVCzGvW+ZQoXdOfVy0Sh3jtvj5qmuXWrb2NbiRjWe+kzcatOYVuImmVlep4dQIEyePNl8veGGGyzrdYqKhx9+2Hw/btw4iYqKMhO56tmXekbkK6+84t82OjralDWfeOIJE6AVK1ZMkpOTZdiwYfkai3s/VQEAgPsUgJSY13v2gDQ2NlZefvlls+SlSpUq8sUXX8j5IBADAAC2yT7dRLAEe392olkfAADAIWTEAABAWExf4UZkxAAAABxCRgwAAERSr36BQkYMAADAIWTEAACAfUiJWZARAwAAcAgZMQAAYBvmEbMiIwYAAOAQMmIAAMA2zCNmRSAGAABsQ6++FaVJAAAAh5ARAwAA9iElZkFGDAAAwCFkxAAAgG2YvsKKjBgAAIBDyIgBAADbMH1FAcqIZWZmysCBA6VatWoSFxcnl1xyiQwfPly8Xq+TwwIAAAj/jNjo0aNl8uTJMmPGDLn88stl9erV0rFjR0lISJAePXo4OTQAABACnDRZgAKxZcuWSZs2baRVq1bmdtWqVeXdd9+VlStX5rp9enq6WXzS0tJsGysAAAgCIrGCU5q85pprZOHChbJ9+3Zze8OGDbJ06VK59dZbc90+JSXFZMt8S2Jios0jBgAACJOMWP/+/U1Wq2bNmhIdHW16xkaOHCnt27fPdfsBAwZInz59/Lf1sQRjAAC4B9NXFKBAbPbs2TJz5kyZNWuW6RFbv3699OrVSypWrCjJycmnbR8TE2MWAACAcOBoINa3b1+TFWvXrp25XadOHfnll19MCTK3QAwAALhcCKavEPcmxJztETt+/LhERVmHoCXKrKwsx8YEAAAQERmx1q1bm56wypUrm9LkunXrZOzYsdKpUycnhwUAAEKEkyYLUCA2ceJEM6Hrk08+KQcPHjS9YY899pgMGjTIyWEBAACEfyBWokQJGT9+vFkAAEAEICVmwbUmAQCAbZi+ogA16wMAAEQyMmIAAMA2nhBMX+Fxb0KMjBgAAIBTyIgBAADb0KtvRUYMAADAIWTEAACAfUiJWZARAwAAcAgZMQAAYBvmEbMiEAMAAPZWJoM9fYW4F6VJAAAAh5ARAwAAtqFX34qMGAAAgEPIiAEAANtwiSMrMmIAAAAOISMGAABsRJdY2AViF5aIkfj4GHGTrfuOiBtdXile3CrLK67kcWnO3et16QF38TF3qy0v3C5uVaPPXHGTrJPHnR4CwjEQAwAA7kCPmBWBGAAAsA2FSSua9QEAABxCRgwAANiG0qQVGTEAAACHkBEDAAC28fzvv2Dv063IiAEAADiEjBgAALAPp01akBEDAABwCBkxAABgGxJiVgRiAADANkxfYUVpEgAAwCFkxAAAgG2YvsKKjBgAAIBDyIgBAAD70K1vQUYMAADAIWTEAACAbUiIWZERAwAAcAgZMQAAYBvmEbMiEAMAADYK/vQV4uLiJKVJAAAAh5ARAwAAtqE0aUVGDAAAwCEEYgAAAA4hEAMAAHAIPWIAAMA29IhZkREDAABwCBkxAABg8yxiwU1heVw8jxiBGAAAsA2lSStKkwAAAA4hIwYAAGyjySsucPR/yIgBAAA4hIwYAACwDykxCzJiAAAADiEjBgAAbMP0FVZkxAAAABxCRgwAANiGecSsyIgBAAA4hIwYAACwDSdNWhGIAQAA+xCJWVCaBAAAcAiBGAAAsH36imD/dy5efvllqVq1qsTGxkrjxo1l5cqVYjcCMQAAEHHef/996dOnjwwePFjWrl0rV1xxhbRs2VIOHjxo6zgIxAAAgO3TVwR7ya+xY8dKly5dpGPHjpKUlCRTpkyRokWLyhtvvCF2cnWzvtfrNV+PHEkTtzl65Ii4UZr7DrVf1l9vF9eJjvK4+vfTjTwunZTIrcfcpcM2sk4eFzeO18n3SloIPkjS/rfPnPuOiYkxS04nT56UNWvWyIABA/zroqKipHnz5rJ8+XKxk6sDsSP/C2ZqXlLF6aEAAOCqz8+EhARbn7NIkSJSvnx5qV4tMST7L168uCQmWvetZcchQ4actu0ff/whmZmZUq5cOct6vb1161axk6sDsYoVK8qePXukRIkSQf8LVqNq/YHq/uPj44O6b+SOY24vjre9ON7245ifTjNhGoTp56fdtCF+165dJhsVqtfmyREL5JYNK2hcHYhpGrFSpUohfQ795eUX2F4cc3txvO3F8bYfx9zK7kxYzmBMF6ddcMEFEh0dLQcOHLCs19uatbMTzfoAACCiFClSRBo2bCgLFy70r8vKyjK3mzRpYutYXJ0RAwAAOBc6dUVycrI0atRIrrrqKhk/frwcO3bMnEVpJwKxPGhdWZv83FBfDhccc3txvO3F8bYfxxxncv/998vvv/8ugwYNkv3790u9evVk/vz5pzXwh5rH69bznQEAAFyOHjEAAACHEIgBAAA4hEAMAADAIQRiAAAADiEQy8PLL78sVatWNRPPNW7cWFauXOn0kMJSSkqKXHnllebqCGXLlpW2bdvKtm3bnB5WxHj++efNTNS9evVyeihhbe/evdKhQwcpU6aMxMXFSZ06dWT16tVODyss6WVrBg4cKNWqVTPH+pJLLpHhw4e79jqcCH8EYrl4//33zfwietrz2rVr5YorrpCWLVvKwYMHnR5a2Fm8eLF07dpVVqxYIQsWLJCMjAxp0aKFmcsFobVq1Sp59dVXpW7duk4PJawdOnRImjZtKoULF5Yvv/xSNm/eLC+99JKUKlXK6aGFpdGjR8vkyZNl0qRJsmXLFnN7zJgxMnHiRKeHBuSK6StyoRkwzdLoL7Jvtl29Xln37t2lf//+Tg8vrOmcLpoZ0wDtuuuuc3o4Yevo0aPSoEEDeeWVV2TEiBFm/hydzBDBp/9m/PDDD/L99987PZSIcPvtt5t5oF5//XX/urvvvttkx9555x1HxwbkhoxYDnox0jVr1kjz5s0t17TU28uXL3d0bJEgNTXVfC1durTTQwlrmoVs1aqV5X2O0Jg7d66Zufvee+81f2TUr19fpk2b5vSwwtY111xjLlOzfft2c3vDhg2ydOlSufXWW50eGpArZtbP4Y8//jA9Bjln1tXbW7dudWxckUAzj9qrpGWc2rVrOz2csPXee++ZkruWJhF6O3fuNKUybXf4xz/+YY57jx49zLXu9PIqCH4GMi0tTWrWrGku6qz/no8cOVLat2/v9NCAXBGIoUBlaX766Sfz1ytCY8+ePdKzZ0/Tj6cnosCePzA0IzZq1ChzWzNi+j6fMmUKgVgIzJ49W2bOnCmzZs2Syy+/XNavX2/+wKtYsSLHGwUSgVgOF1xwgfkr6sCBA5b1ert8+fKOjSvcdevWTebNmydLliyRSpUqOT2csKVldz3pRPvDfDRjoMddeyLT09PN+x/BU6FCBUlKSrKsq1Wrlnz00UeOjSmc9e3b12TF2rVrZ27rGaq//PKLOUObQAwFET1iOWi5oGHDhqbHIPtftHq7SZMmjo4tHOm5IhqEzZkzRxYtWmROOUfoNGvWTDZu3GiyBL5FszVattHvCcKCT0vtOadk0f6lKlWqODamcHb8+HHT15udvq/133GgICIjlgvt5dC/nPQD6qqrrjJnk+l0Ch07dnR6aGFZjtQSwqeffmrmEtu/f79Zn5CQYM5yQnDpMc7Zf1esWDEzvxV9eaHRu3dv00Cupcn77rvPzEk4depUsyD4WrdubXrCKleubEqT69atk7Fjx0qnTp2cHhqQK6avyIOWaV544QUTGOip/RMmTDDTWiC4dDLR3EyfPl0efvhh28cTiW644QamrwgxLbsPGDBAduzYYbK++sdely5dnB5WWDpy5IiZ0FWz7FqG196wBx54QAYNGmQqHkBBQyAGAADgEHrEAAAAHEIgBgAA4BACMQAAAIcQiAEAADiEQAwAAMAhBGIAAAAOIRADAABwCIEYAACAQwjEgAilVy5o27atZYb9Xr162T6O7777zlxh4fDhw3luo/d/8sknAe9zyJAh5moB5+Pf//63eV69BicAhAqBGFDAgiP98NdFL8dy6aWXyrBhw+TUqVMhf+6PP/5Yhg8fHrTgCQBwdlz0GyhgbrnlFnOtzfT0dPniiy/MhdELFy5srlWY08mTJ4N2/bzSpUsHZT8AgMCREQMKmJiYGClfvrxUqVJFnnjiCWnevLnMnTvXUk4cOXKkuZhxjRo1zPo9e/bIfffdJyVLljQBVZs2bUxpzSczM9NcaFrvL1OmjPTr109yXmY2Z2lSA8FnnnlGEhMTzZg0O/f666+b/d54441mm1KlSpnMmO8C7VlZWZKSkmIubB0XFydXXHGFfPjhh5bn0eDysssuM/frfrKPM1A6Lt1H0aJF5eKLLzYXec7IyDhtu1dffdWMX7fT45Oammq5/7XXXpNatWpJbGys1KxZU1555ZV8jwUAzgeBGFDAacCimS+fhQsXyrZt22TBggUyb948E4C0bNlSSpQoId9//7388MMPUrx4cZNZ8z3upZdekjfffFPeeOMNWbp0qfz5558yZ86cMz7vQw89JO+++65MmDBBtmzZYoIa3a8GNh999JHZRsfx22+/yT//+U9zW4Owt956S6ZMmSKbNm2S3r17S4cOHWTx4sX+gPGuu+6S1q1bm96rRx55RPr375/vY6KvVV/P5s2bzXNPmzZNxo0bZ9nm559/ltmzZ8tnn30m8+fPl3Xr1smTTz7pv3/mzJkyaNAgE9Tq6xs1apQJ6GbMmJHv8QDAOfMCKDCSk5O9bdq0Md9nZWV5FyxY4I2JifE+/fTT/vvLlSvnTU9P9z/m7bff9taoUcNs76P3x8XFeb/66itzu0KFCt4xY8b478/IyPBWqlTJ/1zq+uuv9/bs2dN8v23bNk2XmefPzbfffmvuP3TokH/diRMnvEWLFvUuW7bMsm3nzp29DzzwgPl+wIAB3qSkJMv9zzzzzGn7yknvnzNnTp73v/DCC96GDRv6bw8ePNgbHR3t/fXXX/3rvvzyS29UVJT3t99+M7cvueQS76xZsyz7GT58uLdJkybm+127dpnnXbduXZ7PCwDnix4xoIDRLJdmnjTTpaW+v//97+YsQJ86depY+sI2bNhgsj+aJcruxIkT8q9//cuU4zRr1bhxY/99hQoVkkaNGp1WnvTRbFV0dLRcf/31AY9bx3D8+HG5+eabLes1K1e/fn3zvWaeso9DNWnSRPLr/fffN5k6fX1Hjx41JzPEx8dbtqlcubJcdNFFlufR46lZPD1W+tjOnTtLly5d/NvofhISEvI9HgA4VwRiQAGjfVOTJ082wZb2gWnQlF2xYsUstzUQadiwoSm15XThhReeczk0v3Qc6vPPP7cEQEp7zIJl+fLl0r59exk6dKgpyWrg9N5775nya37HqiXNnIGhBqAAYBcCMaCA0UBLG+MD1aBBA5MhKlu27GlZIZ8KFSrIjz/+KNddd50/87NmzRrz2Nxo1k2zR9rbpScL5OTLyOlJAD5JSUkm4Nq9e3eemTRtjPedeOCzYsUKyY9ly5aZExmeffZZ/7pffvnltO10HPv27TPBrO95oqKizAkO5cqVM+t37txpgjoAcArN+oDLaSBxwQUXmDMltVl/165dZp6vHj16yK+//mq26dmzpzz//PNmUtStW7eapvUzzQFWtWpVSU5Olk6dOpnH+Papze9KAyE9W1LLqL///rvJMGm57+mnnzYN+trwrqW/tWvXysSJE/0N8I8//rjs2LFD+vbta0qEs2bNMk33+VG9enUTZGkWTJ9DS5S5nXigZ0Lqa9DSrR4XPR565qSekao0o6YnF+jjt2/fLhs3bjTThowdOzZf4wGA80EgBricTs2wZMkS0xOlZyRq1kl7n7RHzJche+qpp+TBBx80gYn2SmnQdOedd55xv1oeveeee0zQplM7aC/VsWPHzH1aetRARs941OxSt27dzHqdEFbPPNQAR8ehZ25qqVKns1A6Rj3jUoM7ndpCz67UsxXz44477jDBnj6nzp6vGTJ9zpw0q6jH47bbbpMWLVpI3bp1LdNT6BmbOn2FBl+aAdQsngaFvrECgB082rFvyzMBAADAgowYAACAQwjEAAAAHEIgBgAA4BACMQAAAIcQiAEAADiEQAwAAMAhBGIAAAAOIRADAABwCIEYAACAQwjEAAAAHEIgBgAAIM74/4WG9fgugIk9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load MobileNetV2 pre-trained on ImageNet\n",
    "mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Freeze all mobilenet_v2 parameters for faster training (optional)\n",
    "## This prevents us from training our entire model. Let us now add the last layer which interests us for our problem\n",
    "for param in mobilenet_v2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier's last layer with the 10 classes for CIFAR-10\n",
    "mobilenet_v2.classifier[1] = nn.Linear(mobilenet_v2.classifier[1].in_features, 10)\n",
    "\n",
    "# Using the existing CIFAR-10 dataset and resizing the images to 224x224.\n",
    "## This model was trained on 224x224 images (the ImageNet dataset), so it expects input images of that size.\n",
    "transform_mobilenet = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # ImageNet stats\n",
    "])\n",
    "\n",
    "# Prepare DataLoaders for MobileNetV2\n",
    "trainset_mobilenet = datasets.CIFAR10(root=str(data_dir), train=True, transform=transform_mobilenet)\n",
    "testset_mobilenet = datasets.CIFAR10(root=str(data_dir), train=False, transform=transform_mobilenet)\n",
    "train_loader_mobilenet = DataLoader(trainset_mobilenet, batch_size=64, shuffle=True)\n",
    "val_loader_mobilenet = DataLoader(testset_mobilenet, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define optimizer for the classifier only: Adam and CrossEntropyLoss, as the model I created\n",
    "optimizer_mobilenet = optim.Adam(mobilenet_v2.classifier[1].parameters(), lr=0.001)\n",
    "criterion_mobilenet = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop (same as my train() function, but for MobileNetV2)\n",
    "history_mobilenet = train(\n",
    "    mobilenet_v2,\n",
    "    train_loader=train_loader_mobilenet,\n",
    "    val_loader=val_loader_mobilenet,\n",
    "    criterion=criterion_mobilenet,\n",
    "    optimizer=optimizer_mobilenet,\n",
    "    scheduler=optim.lr_scheduler.StepLR(optimizer_mobilenet, step_size=7, gamma=0.1),\n",
    "    epochs=30,\n",
    "    patience=3\n",
    ")\n",
    "\n",
    "# Evaluate MobileNetV2 on the test set (use my test_model_evaluation function, adapted for MobileNetV2)\n",
    "def evaluate_mobilenet():\n",
    "    mobilenet_v2.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    mobilenet_v2.to(device)\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_mobilenet:\n",
    "            images = images.to(device)\n",
    "            outputs = mobilenet_v2(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, average='macro')\n",
    "    rec = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    print(f\"MobileNetV2 Accuracy: {acc:.4f}\")\n",
    "    print(f\"MobileNetV2 Precision: {prec:.4f}\")\n",
    "    print(f\"MobileNetV2 Recall: {rec:.4f}\")\n",
    "    print(f\"MobileNetV2 F1-score: {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"MobileNetV2 Confusion Matrix\")\n",
    "    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "        'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.show()\n",
    "\n",
    "evaluate_mobilenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e09cab83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mobilenet_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f167a626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "MobileNetV2                                        [1, 10]                   --\n",
       "‚îú‚îÄSequential: 1-1                                  [1, 1280, 10, 10]         --\n",
       "‚îÇ    ‚îî‚îÄConv2dNormActivation: 2-1                   [1, 32, 150, 150]         --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-1                            [1, 32, 150, 150]         (864)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-2                       [1, 32, 150, 150]         (64)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU6: 3-3                             [1, 32, 150, 150]         --\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-2                       [1, 16, 150, 150]         --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-4                        [1, 16, 150, 150]         (896)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-3                       [1, 24, 75, 75]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-5                        [1, 24, 75, 75]           (5,136)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-4                       [1, 24, 75, 75]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-6                        [1, 24, 75, 75]           (8,832)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-5                       [1, 32, 38, 38]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-7                        [1, 32, 38, 38]           (10,000)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-6                       [1, 32, 38, 38]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-8                        [1, 32, 38, 38]           (14,848)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-7                       [1, 32, 38, 38]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-9                        [1, 32, 38, 38]           (14,848)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-8                       [1, 64, 19, 19]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-10                       [1, 64, 19, 19]           (21,056)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-9                       [1, 64, 19, 19]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-11                       [1, 64, 19, 19]           (54,272)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-10                      [1, 64, 19, 19]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-12                       [1, 64, 19, 19]           (54,272)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-11                      [1, 64, 19, 19]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-13                       [1, 64, 19, 19]           (54,272)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-12                      [1, 96, 19, 19]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-14                       [1, 96, 19, 19]           (66,624)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-13                      [1, 96, 19, 19]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-15                       [1, 96, 19, 19]           (118,272)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-14                      [1, 96, 19, 19]           --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-16                       [1, 96, 19, 19]           (118,272)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-15                      [1, 160, 10, 10]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-17                       [1, 160, 10, 10]          (155,264)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-16                      [1, 160, 10, 10]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-18                       [1, 160, 10, 10]          (320,000)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-17                      [1, 160, 10, 10]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-19                       [1, 160, 10, 10]          (320,000)\n",
       "‚îÇ    ‚îî‚îÄInvertedResidual: 2-18                      [1, 320, 10, 10]          --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-20                       [1, 320, 10, 10]          (473,920)\n",
       "‚îÇ    ‚îî‚îÄConv2dNormActivation: 2-19                  [1, 1280, 10, 10]         --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-21                           [1, 1280, 10, 10]         (409,600)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-22                      [1, 1280, 10, 10]         (2,560)\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU6: 3-23                            [1, 1280, 10, 10]         --\n",
       "‚îú‚îÄSequential: 1-2                                  [1, 10]                   --\n",
       "‚îÇ    ‚îî‚îÄDropout: 2-20                               [1, 1280]                 --\n",
       "‚îÇ    ‚îî‚îÄLinear: 2-21                                [1, 10]                   12,810\n",
       "====================================================================================================\n",
       "Total params: 2,236,682\n",
       "Trainable params: 12,810\n",
       "Non-trainable params: 2,223,872\n",
       "Total mult-adds (M): 563.17\n",
       "====================================================================================================\n",
       "Input size (MB): 1.07\n",
       "Forward/backward pass size (MB): 195.02\n",
       "Params size (MB): 8.95\n",
       "Estimated Total Size (MB): 205.04\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print model summary\n",
    "summary(mobilenet_v2, input_size=(1, 3, 299, 299))  # (batch_size, input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a24d489",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing\n",
    "\n",
    "Start by loading the CIFAR-10 dataset using PyTorch‚Äôs `torchvision.datasets`. This dataset contains 60,000 color images in 10 classes, with 6,000 images per class.  \n",
    "You should apply preprocessing steps to the images before feeding them into your model. The most common steps are:\n",
    "\n",
    "- **Normalization:** Adjusts pixel values so they have a mean and standard deviation close to zero. This helps the model train faster and more reliably.\n",
    "- **Resizing:** CIFAR-10 images are already 32x32 pixels, so resizing is not needed here.\n",
    "- **Augmentation (optional):** You can add random transformations like flipping, cropping, or rotating images to make your model more robust.\n",
    "\n",
    "After preprocessing, use a DataLoader to batch and shuffle your data, which is important for efficient training.\n",
    "\n",
    "It‚Äôs also good practice to visualize a few images and their labels to understand your data and confirm that preprocessing is working as expected.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Model Architecture\n",
    "\n",
    "Design a Convolutional Neural Network (CNN) suitable for image classification.  \n",
    "A typical CNN for CIFAR-10 includes:\n",
    "\n",
    "- **Convolutional layers:** These extract features from the images using filters.\n",
    "- **Pooling layers:** These reduce the spatial size of the feature maps, making computation more efficient and helping the model focus on the most important features.\n",
    "- **Fully connected layers:** After flattening the feature maps, these layers perform the actual classification.\n",
    "\n",
    "Start with a simple architecture:  \n",
    "- A couple of convolutional layers followed by pooling.\n",
    "- One or two fully connected layers at the end.\n",
    "- The final layer should have 10 outputs (one for each class).\n",
    "\n",
    "You can experiment with deeper or more complex architectures later.  \n",
    "Once your model is defined, you‚Äôll be ready to move on to training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220dab3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
