{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64d2715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries for the CIFAR-10 CNN project\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tarfile\n",
    "import torch\n",
    "import torchvision\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from plotly import graph_objects as go\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Configure device for training (GPU if available, otherwise CPU)\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d508bf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already extracted.\n"
     ]
    }
   ],
   "source": [
    "# Download and Extract CIFAR-10 Dataset\n",
    "\n",
    "# CIFAR-10 dataset URL from official source (University of Toronto)\n",
    "cifar10_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "# Create directory structure for storing the dataset\n",
    "data_dir = Path(\"cifar10_data\")  # Main directory for CIFAR-10 data\n",
    "archive_path = data_dir / \"cifar-10-python.tar.gz\"  # Path where compressed file will be saved\n",
    "\n",
    "# Create the data directory if it doesn't exist\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Check if the dataset archive has already been downloaded\n",
    "if not archive_path.exists():\n",
    "    print(\"Downloading CIFAR-10 dataset...\")\n",
    "    # Stream download to handle large files efficiently\n",
    "    response = requests.get(cifar10_url, stream=True)\n",
    "    \n",
    "    # Write the downloaded content to file in chunks (8KB at a time)\n",
    "    with open(archive_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# Define where the extracted files should go\n",
    "extract_dir = data_dir / \"cifar-10-batches-py\"\n",
    "\n",
    "# Check if the dataset has already been extracted\n",
    "if not extract_dir.exists():\n",
    "    print(\"Extracting CIFAR-10 dataset...\")\n",
    "    # Extract the tar.gz archive\n",
    "    with tarfile.open(archive_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=data_dir)  # Extract all files to data_dir\n",
    "    print(\"Extraction complete.\")\n",
    "else:\n",
    "    print(\"Dataset already extracted.\")\n",
    "\n",
    "# At this point, we have:\n",
    "# - cifar10_data/ (main directory)\n",
    "#   - cifar-10-python.tar.gz (compressed archive)\n",
    "#   - cifar-10-batches-py/ (extracted dataset with batch files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95003021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Transform to tensor for normalisation for test set\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# Using values different from ImageNet Stats, calculated with Claude Sonnet 4.5\n",
    "\n",
    "# Transform for the train set (augmentation+normalization)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(15),  # Rotate image randomly\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color adjustments\n",
    "    transforms.RandomResizedCrop(32, scale=(0.8, 1.0), ratio=(0.9, 1.1)),  # Random crop and resize\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # Random blur\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "# Download and load CIFAR-10\n",
    "trainset = datasets.CIFAR10(root=str(data_dir), train=True, transform=transform_train)\n",
    "testset = datasets.CIFAR10(root=str(data_dir), train=False, transform=transform)\n",
    "\n",
    "img, _ = trainset[0]  # Get the first image and its label from the trainset\n",
    "img_tensor = img\n",
    "print(img_tensor.shape)  # Shape: (C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e234bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.49473685..1.5632443].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horse dog airplane cat\n"
     ]
    }
   ],
   "source": [
    "import torchvision as torchvision\n",
    "\n",
    "# Visualize Sample Images from the CIFAR-10 Training Set\n",
    "\n",
    "# Define the list of class names from the trainset\n",
    "classes = trainset.classes\n",
    "\n",
    "def imshow(img):\n",
    "    # Unnormalize the image (reverse the normalization applied during preprocessing)\n",
    "    img = img / 2 + 0.5\n",
    "    # Convert the tensor to a NumPy array for plotting\n",
    "    npimg = img.numpy()\n",
    "    # Rearrange dimensions from (C, H, W) to (H, W, C) for matplotlib\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of 4 random training images and their labels using DataLoader\n",
    "dataiter = iter(DataLoader(trainset, batch_size=4, shuffle=True))\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Display the images in a grid\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# Print the class names for each image in the batch\n",
    "print(' '.join(f'{classes[labels[j]]}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b321fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# conv1d = nn.Conv1d(\n",
    "#     in_channels=3, # number of input channels e.g. a colored image has 3 channels, the output of a conv layer with 64 neurons has 64 channels\n",
    "#     out_channels=16, # the number of output chennels corresponds to the number of filters or number of neurons on the layers\n",
    "#     kernel_size=5, # the width and height of each convolution filter\n",
    "#     stride=1, # the number of steps taken for each movement of the filter\n",
    "#     padding=0 # the number of padding cells around the input\n",
    "# )\n",
    "\n",
    "# Let's create the convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f35f4459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv1: torch.Size([1, 32, 32, 32])\n",
      "After conv2: torch.Size([1, 32, 8, 8])\n",
      "After conv3: torch.Size([1, 64, 8, 8])\n",
      "After conv4: torch.Size([1, 64, 2, 2])\n",
      "After flatten: torch.Size([1, 512])\n",
      "After fc: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Define a Convolutional Neural Network (CNN) class. \n",
    "# 4 layers, with an approach of 1.5 between the 3rd and 4th layers, so it increases channels progressively\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # First convolutional layer: input 3 channels (RGB), output 32 feature maps\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  \n",
    "        \n",
    "        # Second convolutional layer: input 32, output 32 feature maps\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # Third convolutional layer: input 64, output 64 feature maps\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Fourth convolutional layer: input 96, output 128 feature maps\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "\n",
    "        # Adding new layers for better perfomance\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "\n",
    "        # Flatten layer to convert 2D feature maps to 1D feature vector\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Placeholder fully connected layer, upadted after flatten and 10 classes\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        # Sigmoid activation for output (for binary classification)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        ## Not using sigmoid since it has 10 classes and it's not binary\n",
    "    \n",
    "    def forward(self, x, verbose=False):\n",
    "        # Pass input through first conv layer and apply ReLU activation\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        if verbose: print(\"After conv1:\", x.shape)\n",
    "        # Second conv layer + ReLU\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        if verbose: print(\"After conv2:\", x.shape)\n",
    "        # Third conv layer + ReLU\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        if verbose: print(\"After conv3:\", x.shape)\n",
    "        # Fourth conv layer + ReLU\n",
    "        x = torch.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        if verbose: print(\"After conv4:\", x.shape)\n",
    "\n",
    "        x = torch.relu(self.bn5(self.conv5(x)))\n",
    "        x = torch.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool3(x)\n",
    "        #x = self.dropout3(x)\n",
    "\n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout4(x)\n",
    "        if verbose: print(\"After flatten:\", x.shape)\n",
    "        x = self.fc2(x)\n",
    "        if verbose: print(\"After fc:\", x.shape)\n",
    "        return x\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return f\"Total params: {total_params:,}, Trainable: {trainable_params:,}\"\n",
    "\n",
    "# ---- Utility to automatically adjust the Linear layer ---- This is the utility function ----\n",
    "def initialize_model(input_shape=(3, 32, 32)):\n",
    "    # Create a dummy input tensor with the given shape\n",
    "    dummy = torch.zeros(1, *input_shape)  # batch size 1\n",
    "    model = CNN()\n",
    "    # Forward pass through conv layers to determine flatten size\n",
    "    with torch.no_grad():\n",
    "        out = model.conv1(dummy)\n",
    "        out = torch.relu(model.bn1(out))\n",
    "        out = model.conv2(out)\n",
    "        out = torch.relu(model.bn2(out))\n",
    "        out = model.pool1(out)\n",
    "        out = model.dropout1(out)\n",
    "        out = model.conv3(out)\n",
    "        out = torch.relu(model.bn3(out))\n",
    "        out = model.conv4(out)\n",
    "        out = torch.relu(model.bn4(out))\n",
    "        out = model.pool2(out)\n",
    "        out = model.dropout2(out)\n",
    "        out = model.conv5(out)\n",
    "        out = torch.relu(model.bn5(out))\n",
    "        out = model.conv6(out)\n",
    "        out = torch.relu(model.bn6(out))\n",
    "        out = model.pool3(out)\n",
    "        #out = model.dropout3(out)\n",
    "        out = model.flatten(out)\n",
    "        flatten_size = out.shape[1]\n",
    "    # Redefine the fully connected layer with the correct input size\n",
    "    model.fc1 = nn.Linear(flatten_size, 512)\n",
    "    model.fc2 = nn.Linear(512, 10)\n",
    "    return model\n",
    "\n",
    "# Example: initialize for 32x32 input images (to match the test input below)\n",
    "model = initialize_model(input_shape=(3, 32, 32))\n",
    "\n",
    "# Test forward with shape debugging\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "_ = model(x, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f42c8f",
   "metadata": {},
   "source": [
    "## **Why Use Dropout, Pooling, and Batch Normalization?**\n",
    "\n",
    "Let me explain each technique and how they help your CNN:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Batch Normalization** 📊\n",
    "\n",
    "### **What it does:**\n",
    "Normalizes the inputs to each layer so they have mean≈0 and std≈1.\n",
    "\n",
    "### **Why it helps:**\n",
    "- **Faster training:** Networks converge much quicker\n",
    "- **Stable gradients:** Prevents vanishing/exploding gradients\n",
    "- **Less sensitive to initialization:** More robust training\n",
    "- **Slight regularization effect:** Reduces overfitting a bit\n",
    "\n",
    "### **In your model:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce507b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Result:** Your model trains faster and more reliably!\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Dropout** 🎲\n",
    "\n",
    "### **What it does:**\n",
    "Randomly sets 50% of neurons to zero during training.\n",
    "\n",
    "### **Why it helps:**\n",
    "- **Prevents overfitting:** Forces model to not rely on specific neurons\n",
    "- **Better generalization:** Model learns more robust features\n",
    "- **Reduces memorization:** Can't memorize training data patterns\n",
    "\n",
    "### **In your model:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821a013f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Result:** Your model performs better on new, unseen data!\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Pooling** 🏊‍♂️\n",
    "\n",
    "### **What it does:**\n",
    "Reduces spatial dimensions by taking max/average values in small regions.\n",
    "\n",
    "### **Why it helps:**\n",
    "- **Reduces parameters:** Fewer computations needed\n",
    "- **Translation invariance:** Object recognition regardless of position\n",
    "- **Prevents overfitting:** Less detailed features = less memorization\n",
    "- **Computational efficiency:** Smaller feature maps = faster processing\n",
    "\n",
    "### **You're NOT using pooling currently:**\n",
    "Instead, you use **stride=2** which achieves similar dimension reduction:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e12e52",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Visual Example:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1159beec",
   "metadata": {},
   "source": [
    "Input: 32×32×3 image\n",
    "     ↓ conv1 + bn1 + relu (stride=2)\n",
    "   16×16×32 ← Batch norm stabilizes training\n",
    "     ↓ conv2 + bn2 + relu (stride=2)  \n",
    "   8×8×64   ← More batch norm\n",
    "     ↓ conv3 + bn3 + relu (stride=2)\n",
    "   4×4×96   ← Even more batch norm  \n",
    "     ↓ conv4 + bn4 + relu (stride=2)\n",
    "   2×2×128  ← Final batch norm\n",
    "     ↓ flatten\n",
    "   512      ← 1D vector\n",
    "     ↓ dropout (50% neurons zeroed)\n",
    "   512      ← Some neurons randomly off\n",
    "     ↓ fully connected\n",
    "   10       ← Final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be723df",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Summary:**\n",
    "- **Batch Norm:** Makes training faster and more stable\n",
    "- **Dropout:** Prevents overfitting, improves generalization  \n",
    "- **Pooling (or stride=2):** Reduces size, adds translation invariance\n",
    "\n",
    "**Your current approach is excellent!** You're using batch norm and dropout effectively, and stride=2 instead of pooling works great too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538fb07c",
   "metadata": {},
   "source": [
    "After this comment line, there is a **utility function** that automatically calculates the correct input size for the fully connected (Linear) layer.\n",
    "\n",
    "## **The Problem:**\n",
    "When you design a CNN, after all the convolutional and pooling layers, you need to flatten the output and connect it to a fully connected layer. But it's hard to manually calculate what size the flattened output will be after all those transformations.\n",
    "\n",
    "## **The Solution:**\n",
    "The `initialize_model()` function:\n",
    "1. **Creates a dummy input** with the specified shape (e.g., 3×100×100 for a color image)\n",
    "2. **Runs it through all the convolutional layers** to see what size comes out\n",
    "3. **Calculates the flattened size** (how many numbers you get after flattening)\n",
    "4. **Updates the Linear layer** to have the correct input size\n",
    "\n",
    "## **Why This is Useful:**\n",
    "Instead of manually calculating \"after 4 conv layers with stride 2, my 32×32 image becomes X×X size,\" this function does it automatically by actually running a test input through the network and measuring the result.\n",
    "\n",
    "This ensures your fully connected layer has the right input size to connect properly with the convolutional layers above it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ce5ec5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN                                      [1, 10]                   --\n",
       "├─Conv2d: 1-1                            [1, 32, 32, 32]           896\n",
       "├─BatchNorm2d: 1-2                       [1, 32, 32, 32]           64\n",
       "├─Conv2d: 1-3                            [1, 32, 16, 16]           9,248\n",
       "├─BatchNorm2d: 1-4                       [1, 32, 16, 16]           64\n",
       "├─MaxPool2d: 1-5                         [1, 32, 8, 8]             --\n",
       "├─Dropout: 1-6                           [1, 32, 8, 8]             --\n",
       "├─Conv2d: 1-7                            [1, 64, 8, 8]             18,496\n",
       "├─BatchNorm2d: 1-8                       [1, 64, 8, 8]             128\n",
       "├─Conv2d: 1-9                            [1, 64, 4, 4]             36,928\n",
       "├─BatchNorm2d: 1-10                      [1, 64, 4, 4]             128\n",
       "├─MaxPool2d: 1-11                        [1, 64, 2, 2]             --\n",
       "├─Dropout: 1-12                          [1, 64, 2, 2]             --\n",
       "├─Conv2d: 1-13                           [1, 128, 2, 2]            73,856\n",
       "├─BatchNorm2d: 1-14                      [1, 128, 2, 2]            256\n",
       "├─Conv2d: 1-15                           [1, 128, 2, 2]            147,584\n",
       "├─BatchNorm2d: 1-16                      [1, 128, 2, 2]            256\n",
       "├─MaxPool2d: 1-17                        [1, 128, 1, 1]            --\n",
       "├─Flatten: 1-18                          [1, 128]                  --\n",
       "├─Linear: 1-19                           [1, 512]                  66,048\n",
       "├─Dropout: 1-20                          [1, 512]                  --\n",
       "├─Linear: 1-21                           [1, 10]                   5,130\n",
       "==========================================================================================\n",
       "Total params: 359,082\n",
       "Trainable params: 359,082\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 6.02\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.76\n",
       "Params size (MB): 1.44\n",
       "Estimated Total Size (MB): 2.21\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(1, 3, 32, 32))  # (batch_size, channels, height, width)\n",
    "# the input size is the same as the default by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0bc0f",
   "metadata": {},
   "source": [
    "Now that the model is defined we need to define the loss function, the optimizer, and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04aa396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for early stopping\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "        \"\"\"\n",
    "        Early stopping to stop training when validation loss stops improving.\n",
    "        \n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last improvement\n",
    "            min_delta (float): Minimum change to qualify as an improvement\n",
    "            restore_best_weights (bool): Whether to restore model weights from best epoch\n",
    "        \"\"\"\n",
    "        self.patience = patience  # Number of epochs to wait before stopping\n",
    "        self.min_delta = min_delta  # Minimum improvement threshold\n",
    "        self.restore_best_weights = restore_best_weights  # Whether to restore best weights\n",
    "        self.best_loss = None  # Track the best validation loss seen so far\n",
    "        self.counter = 0  # Count epochs without improvement\n",
    "        self.best_weights = None  # Store the best model weights\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Check if training should stop and update internal state.\n",
    "        \n",
    "        Args:\n",
    "            val_loss (float): Current validation loss\n",
    "            model: The PyTorch model being trained\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if training should stop, False if it should continue\n",
    "        \"\"\"\n",
    "        # First epoch - initialize best loss and save weights\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "            \n",
    "        # Check if current loss is better than best loss (by at least min_delta)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            # Improvement found - update best loss, reset counter, save weights\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0  # Reset the patience counter\n",
    "            self.save_checkpoint(model)\n",
    "            \n",
    "        else:\n",
    "            # No improvement - increment counter\n",
    "            self.counter += 1\n",
    "            \n",
    "        # Check if we've exceeded patience limit\n",
    "        if self.counter >= self.patience:\n",
    "            # Restore best weights if requested\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True  # Signal to stop training\n",
    "            \n",
    "        return False  # Continue training\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"\n",
    "        Save the current model weights as the best checkpoint.\n",
    "        \n",
    "        Args:\n",
    "            model: The PyTorch model to save weights from\n",
    "        \"\"\"\n",
    "        # Create a deep copy of the model's state dictionary\n",
    "        # This prevents issues if the original model weights change\n",
    "        self.best_weights = model.state_dict().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0825af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Changing the model so it adds up the early stopping function\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Adding a regularisation technique for avoiding overfitting. Handicap to the loss function.\n",
    "# This is light, common for CNNs\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "# Adding a scheduler. It adjusts automatiquely the learning rate during the training allowing us to use different learning rates in different moments\n",
    "## Changing the scheduler in order to find a better result : scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)\n",
    "\n",
    "# Training loop\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=100, patience=5):\n",
    "    \"\"\"\n",
    "    Function to train a PyTorch model with training and validation datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    model: The neural network model to train.\n",
    "    train_loader: DataLoader for the training dataset.\n",
    "    val_loader: DataLoader for the validation dataset.\n",
    "    criterion: Loss function (e.g., Binary Cross Entropy for classification).\n",
    "    optimizer: Optimization algorithm (e.g., Adam, SGD).\n",
    "    epochs: Number of training epochs (default=100).\n",
    "    \n",
    "    Returns:\n",
    "    history: Dictionary containing loss and accuracy for both training and validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=0.001, restore_best_weights=True)\n",
    "\n",
    "    # Dictionary to store training & validation loss and accuracy over epochs\n",
    "    history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(epochs):  # Loop over the number of epochs\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss, correct = 0, 0  # Initialize total loss and correct predictions\n",
    "        \n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()  # Reset gradients before each batch\n",
    "            outputs = model(inputs)  # Forward pass, outputs shape: [batch, 10]\n",
    "            loss = criterion(outputs, labels)  # labels shape: [batch], int64\n",
    "            loss.backward()  # Backpropagation (compute gradients)\n",
    "            optimizer.step()  # Update model parameters\n",
    "            \n",
    "            total_loss += loss.item()  # Accumulate batch loss\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "        \n",
    "        # Compute average loss and accuracy for training\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation phase (without gradient computation)\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad():  # No need to compute gradients during validation\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                outputs = model(val_inputs)\n",
    "                loss = criterion(outputs, val_labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == val_labels).sum().item()\n",
    "        \n",
    "        # Compute average loss and accuracy for validation\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Store metrics in history dictionary\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        \n",
    "        # Print training progress\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "            # Save best model based on validation accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "            # Save the best model\n",
    "            torch.save({\n",
    "                'epoch': best_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc}, 'best_model_cifar10.pth')\n",
    "            \n",
    "            print(f\"New best model saved! Epoch {best_epoch}, Val Acc: {best_val_acc:.4f}\")\n",
    "\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            print(f\"Best validation loss: {early_stopping.best_loss:.4f}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Best model from epoch {best_epoch}\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    return history  # Return training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae3768d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Detected MobileNetV2 architecture in saved model\n",
      "✅ Model weights loaded successfully\n",
      "✅ Loaded best model from epoch 1\n",
      "Best validation accuracy: 0.7556\n",
      "Best validation loss: 0.7036\n",
      "✅ Model ready for evaluation!\n"
     ]
    }
   ],
   "source": [
    "def load_best_model(model_path='best_model_cifar10.pth'):\n",
    "    \"\"\"Load the best saved model - handles both CNN and MobileNet architectures\"\"\"\n",
    "    \n",
    "    # Check if the model file exists\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model file '{model_path}' not found.\")\n",
    "        print(\"Please run the training cell first to create the saved model.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    # Check what type of model is saved by examining the keys\n",
    "    state_dict_keys = list(checkpoint['model_state_dict'].keys())\n",
    "    \n",
    "    # Determine model type based on key patterns\n",
    "    if any('features.' in key for key in state_dict_keys):\n",
    "        print(\"🔍 Detected MobileNetV2 architecture in saved model\")\n",
    "        # This is a MobileNetV2 model\n",
    "        model = models.mobilenet_v2(pretrained=False)\n",
    "        # Adjust classifier for CIFAR-10\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 10)\n",
    "    elif any('conv1.' in key for key in state_dict_keys):\n",
    "        print(\"🔍 Detected Custom CNN architecture in saved model\")\n",
    "        # This is your custom CNN model\n",
    "        model = initialize_model(input_shape=(3, 32, 32))\n",
    "    else:\n",
    "        print(\"❌ Unknown model architecture in checkpoint\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load the best weights\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(\"✅ Model weights loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model weights: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # Move to appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Print model info\n",
    "    print(f\"✅ Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "    val_acc = checkpoint.get('best_val_acc', 0.0)\n",
    "    if val_acc != 0.0:\n",
    "        print(f\"Best validation accuracy: {val_acc:.4f}\")\n",
    "    else:\n",
    "        print(f\"Best validation accuracy: N/A\")\n",
    "    \n",
    "    val_loss = checkpoint.get('best_val_loss', 'N/A')\n",
    "    if val_loss != 'N/A':\n",
    "        print(f\"Best validation loss: {val_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Best validation loss: N/A\")\n",
    "    \n",
    "    return model, checkpoint\n",
    "\n",
    "# Usage - only try to load if the file exists\n",
    "if os.path.exists('best_model_cifar10.pth'):\n",
    "    best_model, checkpoint_info = load_best_model('best_model_cifar10.pth')\n",
    "    if best_model is not None:\n",
    "        print(\"✅ Model ready for evaluation!\")\n",
    "    else:\n",
    "        print(\"❌ Failed to load model\")\n",
    "else:\n",
    "    print(\"📝 No saved model found. Run training first!\")\n",
    "    best_model, checkpoint_info = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb022979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 50,000\n",
      "Validation samples: 10,000\n",
      "Batch size: 64\n",
      "Training batches: 782\n",
      "Validation batches: 157\n",
      "Epoch [1/30], Loss: 1.7466, Acc: 0.3524, Val Loss: 1.4059, Val Acc: 0.4773, LR: 0.000997\n",
      "💾 New best model saved! Epoch 1, Val Acc: 0.4773\n",
      "Epoch [2/30], Loss: 1.5422, Acc: 0.4372, Val Loss: 1.2838, Val Acc: 0.5332, LR: 0.000989\n",
      "💾 New best model saved! Epoch 2, Val Acc: 0.5332\n",
      "Epoch [3/30], Loss: 1.4396, Acc: 0.4802, Val Loss: 1.1929, Val Acc: 0.5728, LR: 0.000976\n",
      "💾 New best model saved! Epoch 3, Val Acc: 0.5728\n",
      "Epoch [4/30], Loss: 1.3670, Acc: 0.5086, Val Loss: 1.1138, Val Acc: 0.5950, LR: 0.000957\n",
      "💾 New best model saved! Epoch 4, Val Acc: 0.5950\n",
      "Epoch [5/30], Loss: 1.3152, Acc: 0.5330, Val Loss: 1.0706, Val Acc: 0.6173, LR: 0.000933\n",
      "💾 New best model saved! Epoch 5, Val Acc: 0.6173\n",
      "Epoch [6/30], Loss: 1.2621, Acc: 0.5517, Val Loss: 1.0841, Val Acc: 0.6154, LR: 0.000905\n",
      "Epoch [7/30], Loss: 1.2261, Acc: 0.5648, Val Loss: 1.0576, Val Acc: 0.6268, LR: 0.000872\n",
      "💾 New best model saved! Epoch 7, Val Acc: 0.6268\n",
      "Epoch [8/30], Loss: 1.1935, Acc: 0.5804, Val Loss: 0.9952, Val Acc: 0.6520, LR: 0.000835\n",
      "💾 New best model saved! Epoch 8, Val Acc: 0.6520\n",
      "Epoch [9/30], Loss: 1.1617, Acc: 0.5916, Val Loss: 0.9427, Val Acc: 0.6667, LR: 0.000794\n",
      "💾 New best model saved! Epoch 9, Val Acc: 0.6667\n",
      "Epoch [10/30], Loss: 1.1447, Acc: 0.5986, Val Loss: 0.9155, Val Acc: 0.6767, LR: 0.000750\n",
      "💾 New best model saved! Epoch 10, Val Acc: 0.6767\n",
      "Epoch [11/30], Loss: 1.1207, Acc: 0.6059, Val Loss: 0.9081, Val Acc: 0.6769, LR: 0.000704\n",
      "💾 New best model saved! Epoch 11, Val Acc: 0.6769\n",
      "Epoch [12/30], Loss: 1.0981, Acc: 0.6152, Val Loss: 0.9338, Val Acc: 0.6708, LR: 0.000655\n",
      "Epoch [13/30], Loss: 1.0852, Acc: 0.6192, Val Loss: 0.8589, Val Acc: 0.6889, LR: 0.000604\n",
      "💾 New best model saved! Epoch 13, Val Acc: 0.6889\n",
      "Epoch [14/30], Loss: 1.0652, Acc: 0.6273, Val Loss: 0.8571, Val Acc: 0.6995, LR: 0.000553\n",
      "💾 New best model saved! Epoch 14, Val Acc: 0.6995\n",
      "Epoch [15/30], Loss: 1.0556, Acc: 0.6314, Val Loss: 0.8494, Val Acc: 0.6952, LR: 0.000501\n",
      "Epoch [16/30], Loss: 1.0392, Acc: 0.6373, Val Loss: 0.8536, Val Acc: 0.6956, LR: 0.000448\n",
      "Epoch [17/30], Loss: 1.0203, Acc: 0.6424, Val Loss: 0.7931, Val Acc: 0.7210, LR: 0.000397\n",
      "💾 New best model saved! Epoch 17, Val Acc: 0.7210\n",
      "Epoch [18/30], Loss: 1.0112, Acc: 0.6445, Val Loss: 0.8270, Val Acc: 0.7102, LR: 0.000346\n",
      "Epoch [19/30], Loss: 0.9926, Acc: 0.6507, Val Loss: 0.7925, Val Acc: 0.7194, LR: 0.000297\n",
      "Epoch [20/30], Loss: 0.9827, Acc: 0.6561, Val Loss: 0.7992, Val Acc: 0.7210, LR: 0.000251\n",
      "Epoch [21/30], Loss: 0.9733, Acc: 0.6608, Val Loss: 0.7904, Val Acc: 0.7241, LR: 0.000207\n",
      "💾 New best model saved! Epoch 21, Val Acc: 0.7241\n",
      "Epoch [22/30], Loss: 0.9647, Acc: 0.6632, Val Loss: 0.7827, Val Acc: 0.7218, LR: 0.000166\n",
      "Epoch [23/30], Loss: 0.9601, Acc: 0.6629, Val Loss: 0.7680, Val Acc: 0.7286, LR: 0.000129\n",
      "💾 New best model saved! Epoch 23, Val Acc: 0.7286\n",
      "Epoch [24/30], Loss: 0.9446, Acc: 0.6689, Val Loss: 0.7582, Val Acc: 0.7341, LR: 0.000096\n",
      "💾 New best model saved! Epoch 24, Val Acc: 0.7341\n",
      "Epoch [25/30], Loss: 0.9334, Acc: 0.6729, Val Loss: 0.7767, Val Acc: 0.7279, LR: 0.000068\n",
      "Epoch [26/30], Loss: 0.9324, Acc: 0.6740, Val Loss: 0.7457, Val Acc: 0.7367, LR: 0.000044\n",
      "💾 New best model saved! Epoch 26, Val Acc: 0.7367\n",
      "Epoch [27/30], Loss: 0.9332, Acc: 0.6758, Val Loss: 0.7408, Val Acc: 0.7398, LR: 0.000025\n",
      "💾 New best model saved! Epoch 27, Val Acc: 0.7398\n",
      "Epoch [28/30], Loss: 0.9279, Acc: 0.6763, Val Loss: 0.7557, Val Acc: 0.7356, LR: 0.000012\n",
      "Epoch [29/30], Loss: 0.9277, Acc: 0.6764, Val Loss: 0.7631, Val Acc: 0.7332, LR: 0.000004\n",
      "Epoch [30/30], Loss: 0.9205, Acc: 0.6789, Val Loss: 0.7406, Val Acc: 0.7407, LR: 0.000001\n",
      "💾 New best model saved! Epoch 30, Val Acc: 0.7407\n",
      "\n",
      "Training completed!\n",
      "Best model from epoch 30\n",
      "Best validation accuracy: 0.7407\n",
      "Best validation loss: 0.7406\n",
      "🔍 Detected Custom CNN architecture in saved model\n",
      "✅ Model weights loaded successfully\n",
      "✅ Loaded best model from epoch 30\n",
      "Best validation accuracy: 0.7407\n",
      "Best validation loss: 0.7406\n"
     ]
    }
   ],
   "source": [
    "# TRAINING EXECUTION - Set up data loaders and train the model\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "# DataLoader handles batching, shuffling, and loading data efficiently\n",
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True)   # Training: shuffle for better learning\n",
    "val_loader = DataLoader(testset, batch_size=64, shuffle=False)     # Validation: no shuffle needed\n",
    "\n",
    "# Print dataset information for transparency and debugging\n",
    "print(f\"Training samples: {len(trainset):,}\")        # Total number of training images\n",
    "print(f\"Validation samples: {len(testset):,}\")       # Total number of validation images  \n",
    "print(f\"Batch size: 64\")                             # How many images processed at once\n",
    "print(f\"Training batches: {len(train_loader)}\")      # Number of batches in training set\n",
    "print(f\"Validation batches: {len(val_loader)}\")      # Number of batches in validation set\n",
    "\n",
    "# Execute the training process\n",
    "# This calls the train() function defined earlier with all necessary parameters\n",
    "history = train(\n",
    "    model,                    # The CNN model we defined and initialized\n",
    "    train_loader=train_loader,    # DataLoader for training data\n",
    "    val_loader=val_loader,        # DataLoader for validation data\n",
    "    criterion=criterion,          # Loss function (CrossEntropyLoss)\n",
    "    optimizer=optimizer,          # Optimization algorithm (Adam)\n",
    "    scheduler=scheduler,          # Learning rate scheduler (StepLR)\n",
    "    epochs=30,                   # Maximum number of training epochs\n",
    "    patience=5                   # Early stopping patience (stop if no improvement for 5 epochs)\n",
    ")\n",
    "\n",
    "# Load the best model that was automatically saved during training\n",
    "# The train() function saves the model with highest validation accuracy\n",
    "best_model, _ = load_best_model('best_model_cifar10.pth')\n",
    "\n",
    "# At this point:\n",
    "# - 'history' contains training metrics (loss, accuracy) for each epoch\n",
    "# - 'best_model' contains the model weights from the epoch with best validation accuracy\n",
    "# - The model is ready for evaluation and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b47e87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download reference/pre-trained model for comparison\n",
    "\n",
    "url=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/M08-deep-learning/model_stride.pth\"\n",
    "output_path=\"model_stride.pth\"\n",
    "\n",
    "# Download the reference model (only if not already present)\n",
    "if not os.path.exists(output_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(output_path, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "    print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a74e52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference model loaded successfully!\n",
      "Reference model FC1 input features: 128\n"
     ]
    }
   ],
   "source": [
    "# Load the reference model for comparison\n",
    "\n",
    "\n",
    "ckpt = torch.load(\"model_stride.pth\", map_location=\"cpu\")\n",
    "# Use the correct key for the fully connected layer (fc1 or fc2)\n",
    "in_features = ckpt[\"model_state_dict\"][\"fc1.weight\"].shape[1]  # -> 2048 or similar\n",
    "\n",
    "# Create reference model with correct architecture\n",
    "restored_model = CNN()\n",
    "restored_model.fc1 = nn.Linear(in_features, 512)  # match architecture\n",
    "restored_model.fc2 = nn.Linear(512, 10)           # match architecture\n",
    "restored_model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "restored_model.eval()\n",
    "\n",
    "print(f\"Reference model loaded successfully!\")\n",
    "print(f\"Reference model FC1 input features: {in_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82f32ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL COMPARISON ===\n",
      "Your model parameters: 359,082\n",
      "Reference model parameters: 359,082\n",
      "Your model output shape: torch.Size([1, 10])\n",
      "Reference model output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Compare my trained model with the reference model\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(f\"Your model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Reference model parameters: {sum(p.numel() for p in restored_model.parameters()):,}\")\n",
    "\n",
    "# Test both models on a sample\n",
    "with torch.no_grad():\n",
    "    sample_input = torch.randn(1, 3, 32, 32)\n",
    "    your_output = model(sample_input)\n",
    "    ref_output = restored_model(sample_input)\n",
    "    \n",
    "    print(f\"Your model output shape: {your_output.shape}\")\n",
    "    print(f\"Reference model output shape: {ref_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "753a5aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.8764, Acc: 0.7036, Val Loss: 0.7086, Val Acc: 0.7543, LR: 0.001000\n",
      "💾 New best model saved! Epoch 1, Val Acc: 0.7543\n",
      "Epoch [2/30], Loss: 0.7195, Acc: 0.7485, Val Loss: 0.6737, Val Acc: 0.7633, LR: 0.001000\n",
      "💾 New best model saved! Epoch 2, Val Acc: 0.7633\n",
      "Epoch [3/30], Loss: 0.7037, Acc: 0.7540, Val Loss: 0.6718, Val Acc: 0.7685, LR: 0.001000\n",
      "💾 New best model saved! Epoch 3, Val Acc: 0.7685\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m criterion_mobilenet \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Training loop (same as my train() function, but for MobileNetV2)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m history_mobilenet \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmobilenet_v2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader_mobilenet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader_mobilenet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_mobilenet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_mobilenet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStepLR\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer_mobilenet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Evaluate MobileNetV2 on the test set (use my test_model_evaluation function, adapted for MobileNetV2)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_mobilenet\u001b[39m():\n",
      "Cell \u001b[0;32mIn[20], line 48\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, patience)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Reset gradients before each batch\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass, outputs shape: [batch, 10]\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# labels shape: [batch], int64\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagation (compute gradients)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torchvision/models/mobilenetv2.py:174\u001b[0m, in \u001b[0;36mMobileNetV2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torchvision/models/mobilenetv2.py:166\u001b[0m, in \u001b[0;36mMobileNetV2._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# This exists since TorchScript doesn't support inheritance, so the superclass method\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# (this one) needs to have a name other than `forward` that can be accessed in a subclass\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Cannot use \"squeeze\" as batch-size can be 1\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(x, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torchvision/models/mobilenetv2.py:62\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:548\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ironhack/Projects/CNN projet/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:543\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    533\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    534\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load MobileNetV2 pre-trained on ImageNet\n",
    "mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Freeze all mobilenet_v2 parameters for faster training (optional)\n",
    "## This prevents us from training our entire model. Let us now add the last layer which interests us for our problem\n",
    "for param in mobilenet_v2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier's last layer with the 10 classes for CIFAR-10\n",
    "mobilenet_v2.classifier[1] = nn.Linear(mobilenet_v2.classifier[1].in_features, 10)\n",
    "\n",
    "# Using the existing CIFAR-10 dataset and resizing the images to 224x224.\n",
    "## This model was trained on 224x224 images (the ImageNet dataset), so it expects input images of that size.\n",
    "transform_mobilenet = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),  # ImageNet stats\n",
    "])\n",
    "\n",
    "# Prepare DataLoaders for MobileNetV2\n",
    "trainset_mobilenet = datasets.CIFAR10(root=str(data_dir), train=True, transform=transform_mobilenet)\n",
    "testset_mobilenet = datasets.CIFAR10(root=str(data_dir), train=False, transform=transform_mobilenet)\n",
    "train_loader_mobilenet = DataLoader(trainset_mobilenet, batch_size=64, shuffle=True)\n",
    "val_loader_mobilenet = DataLoader(testset_mobilenet, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define optimizer for the classifier only: Adam and CrossEntropyLoss, as the model I created\n",
    "optimizer_mobilenet = optim.Adam(mobilenet_v2.classifier[1].parameters(), lr=0.001)\n",
    "criterion_mobilenet = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop (same as my train() function, but for MobileNetV2)\n",
    "history_mobilenet = train(\n",
    "    mobilenet_v2,\n",
    "    train_loader=train_loader_mobilenet,\n",
    "    val_loader=val_loader_mobilenet,\n",
    "    criterion=criterion_mobilenet,\n",
    "    optimizer=optimizer_mobilenet,\n",
    "    scheduler=optim.lr_scheduler.StepLR(optimizer_mobilenet, step_size=7, gamma=0.1),\n",
    "    epochs=30,\n",
    "    patience=3\n",
    ")\n",
    "\n",
    "# Evaluate MobileNetV2 on the test set (use my test_model_evaluation function, adapted for MobileNetV2)\n",
    "def evaluate_mobilenet():\n",
    "    mobilenet_v2.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    mobilenet_v2.to(device)\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_mobilenet:\n",
    "            images = images.to(device)\n",
    "            outputs = mobilenet_v2(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, average='macro')\n",
    "    rec = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    print(f\"MobileNetV2 Accuracy: {acc:.4f}\")\n",
    "    print(f\"MobileNetV2 Precision: {prec:.4f}\")\n",
    "    print(f\"MobileNetV2 Recall: {rec:.4f}\")\n",
    "    print(f\"MobileNetV2 F1-score: {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"MobileNetV2 Confusion Matrix\")\n",
    "    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "        'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.show()\n",
    "\n",
    "evaluate_mobilenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print model summary\n",
    "summary(mobilenet_v2, input_size=(1, 3, 299, 299))  # (batch_size, input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0076e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mobilenet_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ea941",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mobilenet_v2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# CREATE COMPREHENSIVE MODEL COMPARISON TABLE\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This provides a clear side-by-side comparison of both models\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create comparison data dictionary with model metrics\u001b[39;00m\n\u001b[1;32m      5\u001b[0m comparison_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustom CNN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMobileNetV2\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameters\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m----> 8\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmobilenet_v2\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo be filled after training\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo be filled after training\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Fill with actual values after training\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArchitecture\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6-layer CNN with BatchNorm + Dropout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPre-trained MobileNetV2 (frozen features)\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput Size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m32x32\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m224x224 (resized)\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Strategy\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom scratch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer learning\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m }\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Create and display the comparison table\u001b[39;00m\n\u001b[1;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(comparison_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mobilenet_v2' is not defined"
     ]
    }
   ],
   "source": [
    "# CREATE COMPREHENSIVE MODEL COMPARISON TABLE\n",
    "# This provides a clear side-by-side comparison of both models\n",
    "\n",
    "# Create comparison data dictionary with model metrics\n",
    "comparison_data = {\n",
    "    'Model': ['Custom CNN', 'ResNet18'],\n",
    "    'Parameters': [f\"{sum(p.numel() for p in model.parameters()):,}\", \n",
    "                   f\"{sum(p.numel() for p in resnet18.parameters()):,}\"],\n",
    "    'Accuracy': ['To be filled after training', 'To be filled after training'],  # Fill with actual values after training\n",
    "    'Architecture': ['6-layer CNN with BatchNorm + Dropout', 'Pre-trained ResNet18 (frozen features)'],\n",
    "    'Input Size': ['32x32', '224x224 (resized)'],\n",
    "    'Training Strategy': ['From scratch', 'Transfer learning']\n",
    "}\n",
    "\n",
    "# Create and display the comparison table\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"=\"*70)\n",
    "print(\"                    MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Display parameter difference\n",
    "custom_params = sum(p.numel() for p in model.parameters())\n",
    "resnet_params = sum(p.numel() for p in resnet18.parameters())\n",
    "param_ratio = resnet_params / custom_params\n",
    "\n",
    "print(f\"\\nParameter Analysis:\")\n",
    "print(f\"• ResNet18 has {param_ratio:.1f}x more parameters than Custom CNN\")\n",
    "print(f\"• Custom CNN: {custom_params:,} parameters\")\n",
    "print(f\"• ResNet18: {resnet_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE TRAINING AND VALIDATION LOSS\n",
    "\n",
    "# Define color palette for consistent, professional visualization\n",
    "color_chart = [\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    "\n",
    "# Create interactive line plot using Plotly\n",
    "fig = go.Figure(data=[\n",
    "                      go.Scatter(\n",
    "                          y=history[\"loss\"],           # Training loss values from history\n",
    "                          name=\"Training loss\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[0]      # Blue color for training\n",
    "                          )),\n",
    "                      go.Scatter(\n",
    "                          y=history[\"val_loss\"],       # Validation loss values from history\n",
    "                          name=\"Validation loss\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[1]      # Teal color for validation\n",
    "                          ))\n",
    "])\n",
    "\n",
    "# Configure plot layout and labels\n",
    "fig.update_layout(\n",
    "    title='Training and val loss across epochs',\n",
    "    xaxis_title='epochs',\n",
    "    yaxis_title='Cross Entropy'    \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61374745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE TRAINING AND VALIDATION ACCURACY\n",
    "\n",
    "# Use same color palette for consistency\n",
    "color_chart = [\"#4B9AC7\", \"#4BE8E0\", \"#9DD4F3\", \"#97FBF6\", \"#2A7FAF\", \"#23B1AB\", \"#0E3449\", \"#015955\"]\n",
    "\n",
    "# Create interactive line plot for accuracy metrics\n",
    "fig = go.Figure(data=[\n",
    "                      go.Scatter(\n",
    "                          y=history[\"accuracy\"],       # Training accuracy values from history\n",
    "                          name=\"Training accuracy\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[0]      # Blue for training\n",
    "                          )),\n",
    "                      go.Scatter(\n",
    "                          y=history[\"val_accuracy\"],   # Validation accuracy values from history\n",
    "                          name=\"Validation accuracy\",\n",
    "                          mode=\"lines\",\n",
    "                          marker=dict(\n",
    "                              color=color_chart[1]      # Teal for validation\n",
    "                          ))\n",
    "])\n",
    "\n",
    "# Configure plot layout and labels\n",
    "fig.update_layout(\n",
    "    title='Training and val accuracy across epochs',\n",
    "    xaxis_title='epochs',\n",
    "    yaxis_title='Cross Entropy'    \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc048b7",
   "metadata": {},
   "source": [
    "THE RESULTS ARE NOT BAD ALREADY."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20e6b2",
   "metadata": {},
   "source": [
    "You can detect **overfitting** or **underfitting** by analyzing your training and validation (test) loss and accuracy curves:\n",
    "\n",
    "---\n",
    "\n",
    "### **Overfitting**\n",
    "- **Training loss** keeps decreasing (model fits training data very well).\n",
    "- **Validation loss** stops decreasing (or starts increasing), and **validation accuracy** plateaus or drops.\n",
    "- **Gap**: There is a large gap between high training accuracy and much lower validation accuracy.\n",
    "\n",
    "**What it means:**  \n",
    "Your model is memorizing the training data but not generalizing to new data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Underfitting**\n",
    "- **Both training and validation loss** are high, and **both accuracies** are low.\n",
    "- The model cannot fit the training data well.\n",
    "\n",
    "**What it means:**  \n",
    "Your model is too simple or not trained enough to capture the patterns in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Check in Your Project**\n",
    "- Look at the `history[\"loss\"]` (training loss) and `history[\"val_loss\"]` (validation loss) curves.\n",
    "- If you plot them (as you do with Plotly), check for:\n",
    "  - **Overfitting:** Training loss goes down, validation loss goes up or stagnates.\n",
    "  - **Underfitting:** Both losses stay high, and accuracy is low.\n",
    "\n",
    "---\n",
    "\n",
    "**Tip:**  \n",
    "- Overfitting: Try more regularization (dropout, weight decay), or get more data.\n",
    "- Underfitting: Try a more complex model, train longer, or reduce regularization.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "- **Big gap, low val accuracy:** Overfitting  \n",
    "- **Both low accuracy:** Underfitting  \n",
    "- **Both high accuracy, low gap:** Good fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b562ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE MODEL EVALUATION ON TEST SET\n",
    "# This function evaluates the trained model's performance using multiple metrics\n",
    "\n",
    "def test_model_evaluation():\n",
    "    # Prepare DataLoader for test set evaluation\n",
    "    test_loader = DataLoader(testset, batch_size=128, shuffle=False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = best_model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, batch norm updates)\n",
    "\n",
    "    # Initialize lists to store all predictions and true labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Perform inference without gradient computation (saves memory and speeds up evaluation)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)  # Forward pass through model\n",
    "            _, preds = torch.max(outputs, 1)  # Get predicted class indices\n",
    "            all_preds.extend(preds.cpu().numpy())  # Store predictions\n",
    "            all_labels.extend(labels.cpu().numpy())  # Store true labels\n",
    "\n",
    "    # Calculate comprehensive evaluation metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)  # Overall accuracy\n",
    "    prec = precision_score(all_labels, all_preds, average='macro')  # Macro-averaged precision\n",
    "    rec = recall_score(all_labels, all_preds, average='macro')  # Macro-averaged recall\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')  # Macro-averaged F1-score\n",
    "\n",
    "    # Display evaluation results\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "    # Generate and visualize confusion matrix to show per-class performance\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Custom CNN Confusion Matrix\")\n",
    "    \n",
    "    # Add class labels to make the confusion matrix more readable\n",
    "    classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.tight_layout()  # Adjust layout to prevent label cutoff\n",
    "    plt.show()\n",
    "\n",
    "    # Sanity check: ensure model performs better than random guessing (10% for 10 classes)\n",
    "    assert acc > 0.1, \"Model accuracy is not better than random guessing.\"\n",
    "\n",
    "# Execute the evaluation function\n",
    "test_model_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa21433",
   "metadata": {},
   "source": [
    "What is a Confusion Matrix?\n",
    "A confusion matrix is a table that shows how well your model is performing on each class.\n",
    "\n",
    "Rows: Actual (true) classes\n",
    "Columns: Predicted classes\n",
    "Each cell [i, j] shows how many images of class i were predicted as class j.\n",
    "How to Read It\n",
    "Diagonal cells (top-left to bottom-right): Correct predictions for each class. Higher numbers here are good!\n",
    "Off-diagonal cells: Misclassifications. For example, if row 3, column 5 is high, your model often confuses class 3 as class 5.\n",
    "What to Look For\n",
    "Perfect classifier: Only diagonal cells are nonzero.\n",
    "If a row has many non-diagonal values: The model struggles to classify that true class.\n",
    "If a column has many non-diagonal values: The model often predicts that class incorrectly for other true classes (over-predicting).\n",
    "\n",
    "How to Use This\n",
    "Find weaknesses: Which classes are most confused? (e.g., cats vs. dogs)\n",
    "Improve model: Consider more data, augmentation, or architecture changes for confused classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6f9b07",
   "metadata": {},
   "source": [
    "Choose a Pre-trained Model\n",
    "\n",
    "Since I am looking for a pre-trained model that is efficient and well-balanced, I have chosen ResNet18. ResNet18 provides an excellent balance between model complexity and performance - it's deeper than simple architectures but not as heavy as larger ResNets. The residual connections help with training stability and gradient flow, making it particularly suitable for transfer learning tasks.\n",
    "\n",
    "ResNet18 is specifically good for image classification tasks like CIFAR-10 because:\n",
    "- **Residual connections**: Help with gradient flow and enable deeper networks\n",
    "- **Balanced architecture**: 18 layers provide good feature extraction without excessive parameters\n",
    "- **Transfer learning friendly**: Pre-trained ImageNet features transfer well to CIFAR-10\n",
    "- **Training efficiency**: Faster to train than larger architectures like ResNet50/101\n",
    "\n",
    "Although pre-trained on ImageNet (224×224 images), it transfers well to CIFAR-10. The learned features (edges, textures, patterns) are still relevant despite the size difference.\n",
    "\n",
    "At first, I considered MobileNetV2 for efficiency, but ResNet18 offers better performance while still being computationally reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a24d489",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing\n",
    "\n",
    "Start by loading the CIFAR-10 dataset using PyTorch’s `torchvision.datasets`. This dataset contains 60,000 color images in 10 classes, with 6,000 images per class.  \n",
    "You should apply preprocessing steps to the images before feeding them into your model. The most common steps are:\n",
    "\n",
    "- **Normalization:** Adjusts pixel values so they have a mean and standard deviation close to zero. This helps the model train faster and more reliably.\n",
    "- **Resizing:** CIFAR-10 images are already 32x32 pixels, so resizing is not needed here.\n",
    "- **Augmentation (optional):** You can add random transformations like flipping, cropping, or rotating images to make your model more robust.\n",
    "\n",
    "After preprocessing, use a DataLoader to batch and shuffle your data, which is important for efficient training.\n",
    "\n",
    "It’s also good practice to visualize a few images and their labels to understand your data and confirm that preprocessing is working as expected.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Model Architecture\n",
    "\n",
    "Design a Convolutional Neural Network (CNN) suitable for image classification.  \n",
    "A typical CNN for CIFAR-10 includes:\n",
    "\n",
    "- **Convolutional layers:** These extract features from the images using filters.\n",
    "- **Pooling layers:** These reduce the spatial size of the feature maps, making computation more efficient and helping the model focus on the most important features.\n",
    "- **Fully connected layers:** After flattening the feature maps, these layers perform the actual classification.\n",
    "\n",
    "Start with a simple architecture:  \n",
    "- A couple of convolutional layers followed by pooling.\n",
    "- One or two fully connected layers at the end.\n",
    "- The final layer should have 10 outputs (one for each class).\n",
    "\n",
    "You can experiment with deeper or more complex architectures later.  \n",
    "Once your model is defined, you’ll be ready to move on to training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f50529f",
   "metadata": {},
   "source": [
    "Looking at your Project.ipynb notebook, I can see you have a comprehensive CNN implementation with training history and evaluation metrics. To properly assess whether your results show overfitting or underfitting, I need to examine the training curves and performance metrics.\n",
    "\n",
    "Let me check if you have the training history results displayed in your notebook: \n",
    "\n",
    "Retrieving Notebook summary.\n",
    "\n",
    "I can see that your notebook has been executed and contains outputs. Let me look at the specific cells that would show your training results - particularly the training/validation curves and evaluation metrics. \n",
    "\n",
    "Read output of [](vscode-notebook-cell:/Users/enriqueestevezalvarez/Documents/Ironhack/Projects/CNN%20projet/Project.ipynb#X24sZmlsZQ%3D%3D)\n",
    "\n",
    "Read output of [](vscode-notebook-cell:/Users/enriqueestevezalvarez/Documents/Ironhack/Projects/CNN%20projet/Project.ipynb#X36sZmlsZQ%3D%3D)\n",
    "\n",
    "Perfect! Now I can analyze your results. Based on your training output and evaluation metrics, here's my assessment:\n",
    "\n",
    "## 📊 **Your Results Analysis: GOOD PERFORMANCE, NO OVERFITTING!**\n",
    "\n",
    "### ✅ **Your Results Are Actually GOOD:**\n",
    "\n",
    "**Final Performance:**\n",
    "- **Test Accuracy: 73.51%** - This is solid for CIFAR-10!\n",
    "- **Precision: 73.18%**\n",
    "- **Recall: 73.51%** \n",
    "- **F1-Score: 73.23%**\n",
    "\n",
    "### 🎯 **No Signs of Overfitting - Here's Why:**\n",
    "\n",
    "**1. Training vs Validation Gap Analysis:**\n",
    "- **Final Training Accuracy: ~66.9%**\n",
    "- **Final Validation Accuracy: 73.5%**\n",
    "- **Gap: Actually NEGATIVE** (validation is higher than training!)\n",
    "\n",
    "**2. Loss Behavior:**\n",
    "- Training loss: Steadily decreasing\n",
    "- Validation loss: Also decreasing and stabilizing\n",
    "- No divergence between training and validation curves\n",
    "\n",
    "**3. Healthy Learning Pattern:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b37ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 22: Train Acc: 66.7%, Val Acc: 73.5%\n",
    "Epoch 23: Train Acc: 66.9%, Val Acc: 73.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beac15f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 📈 **What This Means:**\n",
    "\n",
    "**✅ GOOD GENERALIZATION:**\n",
    "- Your model generalizes BETTER to unseen data than it memorizes training data\n",
    "- This is the opposite of overfitting - it's actually ideal!\n",
    "\n",
    "**✅ PROPER REGULARIZATION:**\n",
    "- Your dropout (0.25, 0.5) and weight decay are working perfectly\n",
    "- BatchNorm is helping with stable training\n",
    "- Early stopping prevented any potential overfitting\n",
    "\n",
    "**✅ CIFAR-10 BENCHMARK COMPARISON:**\n",
    "- 73.5% is respectable for a custom CNN on CIFAR-10\n",
    "- Random guessing = 10%\n",
    "- Your model = 73.5%\n",
    "- You're doing **7.3x better than random!**\n",
    "\n",
    "### 🏆 **Why Your Results Are Good:**\n",
    "\n",
    "1. **No overfitting signs** - validation performs better than training\n",
    "2. **Balanced metrics** - precision, recall, and F1 are all consistent\n",
    "3. **Early stopping worked** - stopped at epoch 23 when improvement plateaued\n",
    "4. **Confusion matrix** shows reasonable performance across all classes\n",
    "\n",
    "### 💡 **Room for Improvement (Optional):**\n",
    "- More data augmentation\n",
    "- Deeper network architecture\n",
    "- Fine-tuning hyperparameters\n",
    "- But your current results are already solid!\n",
    "\n",
    "**Conclusion: Your model is well-trained with good generalization. No overfitting, no underfitting - just solid performance! 🌟**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
